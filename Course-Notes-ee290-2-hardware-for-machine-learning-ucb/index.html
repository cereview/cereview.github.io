<!DOCTYPE html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163163196-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163163196-1');
</script>
<!-- The following line adds the favicon -->
<link rel="shortcut icon" href="https://advancedquantumalgorithms.github.io/favicon.png" />
<html lang="en">
<title>Course Notes: EE290-2 Hardware For Machine Learning | Amitabh&#39;s Weblog</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Notes for the UC Berkeley Course, EE290-2 Hardware for Machine Learning, Spring 2020. Lecturer: Prof. Dr. Sophia Shao Main topics: Deep Neural Networks Quant...">
<meta name="author" content="Amitabh Yadav">
<meta name="generator" content="Jekyll v4.0.1">
<link rel="canonical" href="http://localhost:4000/Course-Notes-ee290-2-hardware-for-machine-learning-ucb/">
<link href='https://fonts.googleapis.com/css?family=Playfair Display' rel='stylesheet'>

<link rel="stylesheet" href="/assets/css/index.css">

<link rel="stylesheet" href="/assets/css/classes.css">
<link rel="stylesheet" href="/assets/css/sidebar.css" media="screen and (min-width: 70em)">
<link rel="alternate" href="/feed.xml" type="application/atom+xml" title="Amitabh's Weblog">

<link rel="stylesheet" href="/assets/katex/katex.min.css">
<script defer src="/assets/katex/katex.min.js"></script>
<script defer src="/assets/katex/contrib/mathtex-script-type.min.js"></script>



</head>
<body>
<header class="icons">
  
    <a href="/" class="title">Amitabh&#39;s Weblog</a>
  
  
    <div class="hidden description"><font face="Playfair Display" size="2">C E R E V I E W</font></div>
  
  <hr>
  
    
  <nav>
  <a aria-label="Home" href="/" ><span aria-hidden="true" >Home</span></a>
  <a aria-label="Categories" href="/categories/" ><span aria-hidden="true" >Categories</span></a>
  <a aria-label="About" href="/about/" ><span aria-hidden="true" >About</span></a>
  
  </nav>


    

  
</header>

<article>
  <header>
  
    <div class="categories">MACHINE LEARNING HARDWARE</div>
  
  <h1><a href="/Course-Notes-ee290-2-hardware-for-machine-learning-ucb/">Course Notes: EE290-2 Hardware For Machine Learning</a></h1><time datetime="2020-04-15T00:00:00+02:00">April 15, 2020</time> 
</header>

  <p>Notes for the UC Berkeley Course, EE290-2 Hardware for Machine Learning, Spring 2020.</p>

<p>Lecturer: Prof. Dr. Sophia Shao</p>

<p>Main topics:</p>
<ul>
  <li>Deep Neural Networks</li>
  <li>Quantization</li>
  <li>Development Platforms</li>
  <li>Kernel Computations
-Dataflows</li>
  <li>State of the art accelerators</li>
  <li>Mapping</li>
  <li>Sparsity</li>
  <li>Hardware/Software Co-Design</li>
  <li>Other NN architectures: RNN, NLP, RM, RL etc</li>
  <li>Advanced Technology</li>
  <li>Training</li>
  <li>ML for Hardware Design, Power Estimation etc.</li>
</ul>

<h2 id="notes-lecture-1">Notes: Lecture 1</h2>
<p>The goal is to make efficient hardware for Machine Learning and Deep Learning applications. The idea is that, we can design massively parallel architecture with hardware level optimizations by exploiting the knowledge of key machine learning algorithm characteristics.</p>

<h4 id="introduction-to-deep-learning">Introduction to Deep Learning</h4>
<p>Deep Learning models have grown vastly in size and require large amounts of compute power.  Such DL kernels run on every compute device such as, Mobile Devices, Drones/Robots, Workstations, Data Centers etc.</p>

<p>The figure below from <a href="https://openai.com/blog/ai-and-compute/">OpenAI</a> shows the  exponential increase of compute for AI training. This has been largely attributed to modern hardware development.
<img src="../images/ee290-2-01.png" alt="OpenAi Compute" /></p>

<p>How hardware intense this could get, as shown in <a href="https://arxiv.org/abs/1711.04325.pdf">this paper</a>, is as follows:
<img src="../images/ee290-2-02.png" alt="Training ResNet50" /></p>

<p><strong>Ques: What is 90epoch?</strong> <em>answer goes here.</em></p>

<p>Moore’s Law is destined to cease in the coming years as the technology size has shrunk down to <a href="https://www.tsmc.com/english/dedicatedFoundry/technology/5nm.htm">5nm</a> so that the gate lengths get as thin as a few layers of atoms. Domain specific accelrators and a vertical integration approach would be the dominant way forward to achieve application speedups and meet the high compute requirements of  Deep Learning Kernels. The following figure shows the teardown of IPhoneXS and the number of specialized kernels. The number of specialized IP blocks such as the Domain Specific Accelerators, Neural Engine etc has grown nearly in a linear fashion <a href="https://ieeexplore.ieee.org/abstract/document/7106399">Ref: Shao et.al. IEEE Micro 2015</a> from abut 9 in the ARM A4 processor in 2010 to about 40 in the ARM A11 in 2017.</p>

<p><img src="../images/ee290-2-03.png" alt="Apple iPhoneXS Teardown" /></p>

<p>Deep Learning Rigs:</p>
<ol>
  <li>NVIDIA Volta V100 GPU:
    <ul>
      <li>21B Transistors at TSMC 12nm FinFET.</li>
      <li>15.7 TFLOPS/s Single Precision-FP (FP32) performance.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></li>
      <li>125 Tensor TFLOP/s of mixed-precision Matrix-Multipl-and-Accumulate.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></li>
      <li>300W</li>
    </ul>
  </li>
  <li>NVIDIA DGX-2 = 16 \(\times\) Tesla V100.
    <ul>
      <li>Power: 10kW</li>
    </ul>
  </li>
  <li>Cerebras: Wafer-Scale Deep Learning
    <ul>
      <li>1.2 Trillion Transistors</li>
      <li>46,225 mm\(^2\) silicon die area.</li>
      <li>400,000 Optimized Cores</li>
      <li>16GB Onchip Memory</li>
      <li>TSMC 16nm</li>
      <li>Challenges:
        <ul>
          <li>Process Variations</li>
          <li>Heating</li>
          <li>Routing/Interconnects, Second-order/higher-order effects etc.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Tesla Self-Driving Computer
    <ul>
      <li>96\(\times\)96 MACs</li>
      <li>32MB SRAM/instance</li>
    </ul>
  </li>
  <li>Google TPU</li>
</ol>

<h4 id="how-to-compare-performance-of-hardware">How to Compare Performance of Hardware?</h4>
<p><a href="https://mlperf.org/">MLPerf</a></p>

<p><em>More on this in later lectures</em></p>

<hr />

<h2 id="notes-lecture-2">Notes: Lecture 2</h2>
<p>AI: The science and engineering of creating intelligent machines. - John McCarthy, 1956.</p>

<p>ML: The field of study that gives the computers the ability to learn without being explicitly programmed. - Arthur Samuel, 1959.</p>

<p>DL: Seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels. - Yoshua Bengio, 2012.</p>

<p>Three waves of NN came before present day scenario: Cybernetics (1940s-60s), Connectionism(1980s-90s) and Deep Learning(2006 - present). The size of dataset increased dramatically which made neural networks possible. Also hardware caught up and we could get better performance from the NN architectures. For example, CIFAR10 has 60,000 of 32\(\times\)32 images: 50k training+10k Test, arranged in 10 classes (eg. airplance, automobile, cat, bird etc) - 6000 images per class. The following figure shows dataset sizes from Goodfellow et.al.</p>

<p><img src="../images/ee290-2-04.png" alt="Dataset Sizes" /></p>

<p>Increasing data-sizes implies Increasing Model Sizes as well. Model Size increase implies increase in:</p>
<ol>
  <li>Number of Neurons</li>
  <li>Number of connections per neuron.</li>
</ol>

<p>This is largely do to the availability of better hardware and this trend, it seems, would continue in the forseeable future. The following figure shows the increasing model sizes based on aforementioned criteria.</p>

<p><img src="../images/ee290-2-05.png" alt="Model size increase trend" /></p>

<h4 id="basics-of-ml-dataset-and-optimization-methods">Basics of ML: Dataset and Optimization methods</h4>
<blockquote>
  <p>“A computer program is said to learn from experience (E) with respect to some task (T) and some performance measure (P), if its
performance on T, as measured by P, improves with experience E.” - Tom Mitchell, 1998.</p>
</blockquote>

<p>For example: Spam e-mail classification:</p>
<ul>
  <li>Task (T): Predict emails as spam or not spam.</li>
  <li>Experience (E): Observe users label emails as spam or not.</li>
  <li>Performance (P): Number of emails that are correctly predicted.</li>
</ul>

<p>ML Algorithms are:</p>
<ol>
  <li>Supervised: Data + labels given. e.g. Classification and Regression.</li>
  <li>Unsupervised: Data only. No labels. e.g. Clustering</li>
  <li>Reinforcement Learning: Interact with environment and learn.</li>
  <li>Self-Supervised Learning: Contact:- Yann LeCun. ;)</li>
</ol>

<p>In nearly all ML algorithms, we can identify T, P and E as follows:</p>
<ul>
  <li>Dataset \(\rightarrow\) Experience(E)</li>
  <li>Cost (loss) function \(\rightarrow\) Performance Measure (P)</li>
  <li>Model + Optimization Method \(\rightarrow\) Task (T)</li>
</ul>

<h5 id="example-linear-regression">Example: Linear Regression</h5>
<p>Linear Regression is supervised learning from a dataset to estimate a value (that may or may not be in the original dataset) based on given datavalues. E.g. find the real-estate sale price (US Dollars) in the given area, given dataset of sale values of different plot sizes in a particular area.</p>

<p>How it works? We have data with Labels.</p>

<p>Partition the data in two: (1) Training Set (to train the model), and (2) Test Set (to test the trained model) and (3) Validation Set.</p>

<p>An overfitting or underfitting of model renders the model inefficient and returns wrong results.</p>

<p>More detailed notes on Linear Regression is available in another blogpost.</p>

<h4 id="basics-of-deep-learning">Basics of Deep Learning</h4>

<h4 id="alexnet">AlexNet</h4>
<p>We have discussed AlexNet in detail in the review post, <a href="../ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-AlexNet/">here</a>.</p>

<hr />
<h2 id="lecture-3-quantization">Lecture 3: Quantization</h2>

<hr />

<h2 id="some-other-interesting-stuff">Some other interesting Stuff:</h2>
<p>Tips for writing paper reviews:</p>
<ol>
  <li>Motivation for the work.</li>
  <li>Proposed Solution.</li>
  <li>Evaluation of the proposed solution.</li>
  <li>Analysis of identified problem, proposed solution and evaluation.</li>
  <li>Future Directions for the research.</li>
  <li>Queries.</li>
</ol>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>What is TFLOP/s? <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>What is Mixed Precision Matrix-Multipl-and-Accumulate? <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  
</article>


  <footer class="related">
  	<div class="previous"><span>Previous Post</span><a href="/Quantization-and-Training-of-Neural-Networks-for-efficient-Integer-Arithmetic-Only-Inference/">Quantization and Training of NN for efficient Integer-Arithmetic-Only Inference</a></div>
	<div class="next"><span>Next Post</span><a href="/cuDNN-Efficient-Primitives-for-Deep-Learning/">cuDNN: Efficient Primitives for Deep Learning</a></div>
  </footer>

<!-- -->
</body>
</html>
