<!DOCTYPE html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163163196-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163163196-1');
</script>
<!-- The following line adds the favicon -->
<link rel="shortcut icon" href="https://advancedquantumalgorithms.github.io/favicon.png" />
<html lang="en">
<title>Quantization and Training of NN for efficient Integer-Arithmetic-Only Inference | Amitabh&#39;s Weblog</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Summary Modern CNN architectures have very high model complexity and demard high computational efficiency. Mobile devices, however, present the challenge to ...">
<meta name="author" content="Amitabh Yadav">
<meta name="generator" content="Jekyll v4.0.1">
<link rel="canonical" href="http://localhost:4000/Quantum-Algorithms-for-Hamiltonian-Simulation/">
<link href='https://fonts.googleapis.com/css?family=Playfair Display' rel='stylesheet'>

<link rel="stylesheet" href="/assets/css/index.css">

<link rel="stylesheet" href="/assets/css/classes.css">
<link rel="stylesheet" href="/assets/css/sidebar.css" media="screen and (min-width: 70em)">
<link rel="alternate" href="/feed.xml" type="application/atom+xml" title="Amitabh's Weblog">

<link rel="stylesheet" href="/assets/katex/katex.min.css">
<script defer src="/assets/katex/katex.min.js"></script>
<script defer src="/assets/katex/contrib/mathtex-script-type.min.js"></script>



</head>
<body>
<header class="icons">
  
    <a href="/" class="title">Amitabh&#39;s Weblog</a>
  
  
    <div class="hidden description"><font face="Playfair Display" size="2">C E R E V I E W</font></div>
  
  <hr>
  
    
  <nav>
  <a aria-label="Home" href="/" ><span aria-hidden="true" >Home</span></a>
  <a aria-label="Categories" href="/categories/" ><span aria-hidden="true" >Categories</span></a>
  <a aria-label="About" href="/about/" ><span aria-hidden="true" >About</span></a>
  
  </nav>


    

  
</header>

<article>
  <header>
  
    <div class="categories">MACHINE LEARNING HARDWARE</div>
  
  <h1><a href="/Quantum-Algorithms-for-Hamiltonian-Simulation/">Quantization and Training of NN for efficient Integer-Arithmetic-Only Inference</a></h1><time datetime="2020-04-20T00:00:00+02:00">April 20, 2020</time> 
</header>

  <h2 id="summary">Summary</h2>
<p>Modern CNN architectures have very high model complexity and demard high computational efficiency. Mobile devices, however, present the challenge to accomodate within limited memory and meeting low latency to maintina user engagement. The cost of performing inference on mobile devices like, smartphones, AR/VR devices, drones etc. bears large computation and memory overhead. In order to have more efficient inferece hardware on mobile devices with integer-only arithmetic hardware, a <em>quantization scheme</em> helps in developing integer-only arithmetic to substitute the more sophesticated floating-point arithmetic hardware. Quantization, therefore, is the tradeoff between accuracy and on-device latency. The task at hand is to reduce model sizes and inference times, with minimal loss to accuracy. This also significantly affects the training procedure and requires the computer architect to maintain end-to-end model accuracy.</p>

<p>This paper presents such a <em>quantization scheme</em> along with a hardware/software co-design training procedure. The improvements are demonstracted on MobileNet (a model family for run-time efficiency) running ImageNet classification and COCO object detection on modern CPUs.</p>

<h4 id="contributions-of-the-paper">Contributions of the paper:</h4>
<ol>
  <li>A <em>quantization scheme</em> to quantize <em>weights</em> and <em>activations</em> as 8-bit integers, and other few parameters (such as, bias vectors) as 32-bit integers. This quantization scheme is derived from Ref.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> suggesting fixed-point arithemetic to accelerate training speed and Ref.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> suggesting 8-bit fixed-point arithemetic to accelerate inference on x86 architecture.</li>
  <li>A <em>quantization inference framework</em> for integer-arithmetic only hardware.</li>
  <li>A co-designed <em>quantization training framework</em> to maintain accuracy of inferece.</li>
  <li>Presents the implementation of the frameworks on<em>MobileNet</em>running on ARM CPUs, to perform classification (ImageNet<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>) and object detection (COCO <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>).</li>
</ol>

<h2 id="details">Details</h2>
<p>The quantization scheme employs an Integer-arithmetic only for Inference; and a Floating-point arithmetic for training. Both representation have a high degree of correlation with each other by separate adoption of the <em>quantization scheme</em> for each.</p>

<p>\(q \rightarrow\) <em>quantized value</em>, denotes bit-representation of values.</p>

<p>\(r \rightarrow\) <em>real value</em>, denotes the actual numerical value.</p>

<p>The integer value to quantized value <em>mapping</em> is given as follows:</p>

\[r=S(q-Z)\]

<p>where, \(S\) and \(Z\) are some constants called <em>quantization parameters</em>. q can be quantized as <em>B</em>-bit integer for <em>B</em>-bit quantization. Here, <em>B</em> is 8-bits. Bias vectors are quantized as 32-bit integers.</p>

<p>Here, a single set of quantization parameters is used for both <em>weights array</em> and <em>activations array</em>. Separate arrays can use separate quantization parameters. The mapping can be implemented as SIMD as opposed to the alternate method of using look-up table, to have better performance.</p>

<h4 id="8-bit-quantization">8-bit Quantization</h4>
<p>\(r=S(q-Z)\)</p>

<p>\(S \rightarrow\) <em>“Scale”</em> is arbitrary positive number. In software, it is a floating-point number just like the real value \(r\), of type <code class="highlighter-rouge">float</code>. Note that for inference, the floating-point quantities need to be eliminated (discussed ahead).</p>

<p>\(Z \rightarrow\) <em>“Zero-point”</em> is the <em>quantized value</em> corresponding to \(0\), and is of the same type as \(q\) i.e. <code class="highlighter-rouge">uint8</code>.</p>

<p>Using the above mapping, 0 is exactly representable in real value as well. NN implementation often has 0-padding of arrays around boundaries. This property of the mapping facilitates in that.</p>

<h4 id="how-to-do-integer-arithmetic-only-matrix-multiplication">How to do Integer-Arithmetic-Only Matrix Multiplication?</h4>
<p>Currently, \(r\) and \(S\) are floating point, and we need to have an integer-arithmetic-only inference scheme using the mapping \(r=S(q-Z)\).</p>

<p>Given, \(N \times N\) matrix composed of real values \(r_1\) and \(r_2\). Their product matrix has values \(r_3 = r_1r_2\). We have \(r_{\alpha}^{i,j}\) such that, \(1 \leq i,j \leq N\), with quantization parameters \((S_\alpha, Z_\alpha)\).</p>

\[r_{\alpha}^{i,j} = S_\alpha(q_{\alpha}^{i,j} - Z_\alpha)\]

<p>By performing matrix multiplication, we have:</p>

\[S_3(q_{3}^{i,k} - Z_3) = \sum_{j=1}^N S_1(q_{1}^{i,j} - Z_1)S_2(q_{2}^{j,k} - Z_2)\]

<p>rewriting,</p>

\[q_{3}^{i,k} = Z_3 + M\sum_{j=1}^N (q_{1}^{i,j} - Z_1)(q_{2}^{j,k} - Z_2)\]

<p>where \(M\) remains the only non-integer constant which can be calculated offline using quantization scales \(S_1, S_2\) and \(S_3\),</p>

\[M := \frac{S_1S_2}{S_3}\]

<p>Empirically, it is determined that 0 \lt M \lt 1, and therefore can be expressed in the normalized form as follows:</p>

\[M = 2^{-n}M_0\]

<p>where, \(M_0 \in (0.5,1]\) and \(n \in \mathbb I^{+}\). The normalized \(M_0\) can be expressed as a fixed-point multiplier e.g. <code class="highlighter-rouge">int16</code> or <code class="highlighter-rouge">int32</code> depending on hardware. E.g. if <code class="highlighter-rouge">int32</code> is used the \(M_0\) will be the value closest to \(2^{31}M_0\). Also, since \(M_0 \gt 0.5\), so \(M_0\) has atleast 30bits of relative accuracy.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup> This way, multiplication operation with \(M_0\) can be expressed as fixed point multiplication and the multiplication with \(2^{-n}\) can be done by bit-shifts with correct round-to-nearest behaviour.</p>

<h4 id="efficient-handing-of-zero-points">Efficient Handing of Zero-points</h4>

<h4 id="implemention-of-typical-fused-layer">Implemention of typical fused layer</h4>

<h3 id="training-with-simulated-quantization">Training with Simulated Quantization</h3>

<h3 id="experiment-and-results">Experiment and Results</h3>

<h2 id="analysis-strengths-and-weaknesses">Analysis: Strengths and Weaknesses</h2>
<ul>
  <li>This paper presents a detailed approach to reducing the inference latency on devices with limited memory and compute power. Unlike using an improper baseline architecture such as AlexNet, GoogleNet etc. to compare which does not reflect the advantage for mobile devices, this paper presents the improvement on MobileNet architectures, which makes the results significantly important for the Computer Architecture community.</li>
  <li>Another important strength of the paper is that if presents the co-design framework and the experimental implementation on prominent mobile CPU for both training and inference using the proposed methodology i.e. the <em>quantization scheme</em>. The benchmarking is done using ImageNet and COCO, which present a useful comparison of accuracy between the tradition FP-hardware and Integer-arithmetic hardware.</li>
  <li>The weakness of the paper is that, it does not discuss about efficient integer-arithmetic representation for varying weight/activation vector sizes. Such as, why was 8-bit for weights and activation chosen, what would be the effect of chosing a higher bit counts, why does bias get 32-bit etc.</li>
</ul>

<h2 id="ideas-and-future-directions">Ideas and Future Directions</h2>

<h2 id="lessons-learned-and-questions">Lessons learned and Questions?</h2>
<p>The only question I’m left pondering is wheter a mathematical model can be derived for comparing the FP vs Integer hardware? i.e. a theoretical comparison of accuracy of FP arithemetic approach against the expected deterioration of the proposed approach, and how much error cane be accomodated without compensating for the overall accuracy of the inference.</p>

<h2 id="some-other-interesting-approaches">Some other interesting approaches</h2>

<p><strong>Reference:</strong> Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H. and Kalenichenko, D., 2018. <em>“Quantization and training of neural networks for efficient integer-arithmetic-only inference”</em>. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2704-2713).</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Gupta, Suyog, et al. “Deep learning with limited numerical precision.” International Conference on Machine Learning. 2015. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao. “Improving the speed of neural networks on CPUs.” (2011). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Deng, Jia, et al. “Imagenet: A large-scale hierarchical image database.” 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Lin, Tsung-Yi, et al. “Microsoft coco: Common objects in context.” European conference on computer vision. Springer, Cham, 2014. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Explaination Required. Further explained in Appendix B. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  
</article>


  <footer class="related">
  	<div class="previous"><span>Previous Post</span><a href="/cuDNN-Efficient-Primitives-for-Deep-Learning/">cuDNN: Efficient Primitives for Deep Learning</a></div>
	<div class="next"></div>
  </footer>

<!-- -->
</body>
</html>
