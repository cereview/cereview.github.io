<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-01-24T18:36:48+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">amitabh yadav</title><subtitle>It&apos;s not right but the thing is, it&apos;s not even wrong!</subtitle><author><name>Amitabh Yadav</name></author><entry><title type="html">The Hardware Lottery</title><link href="http://localhost:4000/posts/2022/the-hardware-lottery/" rel="alternate" type="text/html" title="The Hardware Lottery" /><published>2023-01-24T00:00:00+01:00</published><updated>2023-01-24T00:00:00+01:00</updated><id>http://localhost:4000/posts/2022/the-hardware-lottery</id><content type="html" xml:base="http://localhost:4000/posts/2022/the-hardware-lottery/"><![CDATA[<p>Sara Hooker, “The hardware lottery”, Commun. ACM 64, 12, December 2021, pp. 58-65, https://doi.org/10.1145/3467017</p>

<h4 id="synopsis">Synopsis</h4>
<p>The term, Hardware Lottery, is used here to define the unintentional prejudice observed historically towards certain research ideas in computer science that succeed, not because they were far superior to others but due to how well-suited they were for the available hardware and software of the time, often creating ‘noise’ to the interpretability of new research directions. The domain-specific era is even more prone to this, where the hardware landscape is getting even more fragmented. Figure 1 demonstrates this. Writing software for each system is expensive and even then chances are only those companies would succeed whose technologies can successfully be adopted to commercial applications. Specialised hardware can be replaced with the advent of newer technologies, therefore scalability and adoptability is critical to a hardware development infrastructure.</p>

<h4 id="strengths">Strengths</h4>
<p>The numerous examples from history from Edison’s Phonograph to modern GPUs is an eye opener and provides a broader perspective to think about the larger picture of computing hardware landscape. The presented solutions are agreed upon propositions in the research community. The emphasis on revisiting FPGAs and CGRAs for harmonising hardware development, developing better profiling tools and using AI for design space exploration is something I agree with. Secondly, just like hardware is being evolved to be highly parameterized, parameterized DSLs for reproducing results on different hardware is necessary (functional programming, and a higher abstraction level of application specification can help: e.g. Code Generating (Explainable) AI for hardware backends).</p>

<h4 id="weaknesses">Weaknesses</h4>
<p>The article highlights the issues that cost, limited availability, and the lack of standardisation in hardware, will make it difficult for researchers to replicate or reproduce results. One potential missing element in the article is the discussion that the difficulties in obtaining hardware resources also limits the number of researchers working on certain problems, and slowing down of progress in certain areas. It leads to a lack of diversity in the types of research being conducted, as researchers may be limited to working on problems that can be solved with the hardware they have available (not to mention, Gartner Hype Cycle). Additionally, an obstacle is technology not being available for researchers from underprivileged countries, institutions or backgrounds (IC production is Expensive!). Lastly, it could also be discussed that standardising a set of norms for developers (in academia and industry) would help mitigate the fragmented landscape, e.g. with the open-source hardware platform like RISC-V, and compiler infrastructure like LLVM/MLIR.</p>

<h4 id="thoughts">Thoughts</h4>
<p>Domain-Specific era didn’t dawn until Moore’s Law and Dennard Scaling stopped to apply. It can be worth investigating the next research idea (after-domain specific) and lay a path for its integration in future, which might not apply today for immediate adoption but can be simply available when needed [1, 2]. This can be an expensive endeavour and may not succeed - “being too early is the same as being wrong” - so caution must be exercised. For academic research, some areas include, photonic accelerator, synaptic transistor, memristor array, new memory technology, neuromorphic etc. for hardware; Neurosymbolic AI, Formal Methods etc. for software. Secondly, while it is true presently that having specialised hardware can greatly accelerate the training of deep learning models, there are also other ways to address the issue of creating uniformity in the domain-specific landscape, such as algorithmic research to use general purpose hardware (not everything needs DL), using cloud computing resources, and creating cross-DSL(domain-specific language) compilers, DSE tools and open-source solutions. Lastly, one positive impact in the DS-era would be that small teams and independent researchers would be making breakthroughs with more limited resources through the use of creative solutions and open-source tools.</p>

<h4 id="takeaways">Takeaways</h4>
<p>Hardware development maintained its course ever since the 90s, however software developed fast (number of ML publications since 2005). This led to hardware being abstracted away completely by software. With increasing software complexity, hardware will be a key factor that would dictate the success for a research idea [3]. Whereas for the hardware competing in the ‘bigger is better’ race, history tells us, “a deficiency in any one number of factors dooms an endeavour to failure.” A critical understanding of the hardware/software landscape, their compatibility and co-designability is therefore elemental.</p>
<ul>
  <li>Large new neural networks would need faster development time for corresponding accelerators, some architectures would need even more constrained design than scaling systolic arrays/vector architectures e.g. [4,5,6] and [7].</li>
  <li>Accelerating development time and cost is essential for hardware engineers. Ideas like [8, 9] didn’t succeed because the cost of iteration was too high. For software, developers need to come up with creative solutions to adapt to the current norm of accelerators.</li>
  <li>Looking ahead is essential to progress, ideas respawn when the time is right e.g. the case of deep neural networks in software and [10, 11] for corresponding hardware. Additionally, DNNs may not be the only way forward, at some point a new technology may spawn (e.g. something that better resembles the human brain) and we may have to redo everything. Scaling and Adaptability here needs more emphasis.</li>
  <li>From an economist’s perspective, the law of supply and demand is at play here. In one line, “Necessity is the mother of invention”.</li>
</ul>

<h4 id="favourite-bits">Favourite bits</h4>
<p>“Our own cognitive intelligence is both hardware and software [a domain-specific computer].”</p>

<p>“[accelerators] Happy families are all alike, unhappy families are unhappy in their own way.”</p>

<p>“[bigger is better race] An apt metaphor is that we may be trying to build a ladder to the moon.”</p>

<p>“Scientific progress occurs when there is a confluence of factors which allows scientists to overcome the ‘stickiness’ of the existing paradigm.”</p>

<p>“Registering what differs from our expectations, remains a key catalyst in driving new scientific discoveries.”</p>

<p>“[computer chip is] inscribing words on grains of sand.”</p>

<h4 id="suggested-reading">Suggested Reading</h4>
<p>Science in the age of selfies [12].</p>

<h4 id="case-study-of-isa">Case study of ISA</h4>
<p>Krste Asanovic once said, ‘Don’t make your own ISAs.’ This is partly true in the sense that an incompatible ISA without existing software would never make it to upstream. We have observed the success of two proprietary ISAs so far, x86_64 and ARM32/64, asserting their dominance in desktop/server and mobile markets, respectively due to them being early and developing their own software ecosystem. Other ISAs of the time VAX, MIPS, SPARC etc became only moderately successful (and are now out of circulation). Why is it partly true? Because new systems, like Graphcore and Cerebras are bringing enough performance (in addition to marketing) to the table to compete in the HPC space.</p>

<p>References:
[1] Sutton, R. The bitter lesson, 2019. URL http://www.incompleteideas.net/IncIdeas/BitterLesson.html.</p>

<p>[2] Welling, M. Dowestill need modelsorjust more data and compute?, 2019. URL shorturl. at/qABIY.</p>

<p>[3] Barham, P. and Isard, M. Machine learning systems are stuck in a rut. In Proceedings of the Workshop on Hot Topics in Operating Systems, HotOS ’19, pp. 177–183, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367271. doi: 10.1145/3317550.3321441. URL https://doi.org/10.1145/3317550.3321441.</p>

<p>[4] Hooker, S., Courville, A., Clark, G., Dauphin, Y., and Frome, A. What Do Compressed Deep Neural Networks Forget? arXiv e-prints, art. arXiv:1911.05248, November 2019.</p>

<p>[5] Gale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks, 2019.</p>

<p>[6] Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the Lottery: Making All Tickets Winners. arXiv e-prints, November 2019.</p>

<p>[7] Zhen, D., Yao, Z., Gholami, A., Mahoney, M., and Keutzer, K. Hawq: Hessian aware quantization of neural networks with mixedprecision, 10 2019.</p>

<p>[8] Kingsbury, B., Morgan, N., and Wawrzynek, J. Hipnet-1: A highly pipelined architecture for neural network training, 03 1998.</p>

<p>[9] Sackinger, E., Boser, B. E., Bromley, J., LeCun, Y., and Jackel, L. D. Application of the anna neural network chip to high-speed character recognition. IEEE Transactions on Neural Networks, 3(3):498–505, 1992.</p>

<p>[10] Hinton, G. E. and Anderson, J. A. Parallel Models of Associative Memory. L. Erlbaum Associates Inc., USA, 1989. ISBN 080580269X.</p>

<p>[11] Rumelhart, D. E., McClelland, J. L., and PDP Research Group, C. (eds.). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations. MIT Press, Cambridge, MA, USA, 1986. ISBN 026268053X.</p>

<p>[12] Geman, Donald, and Stuart Geman. “Science in the age of selfies.” Proceedings of the National Academy of Sciences 113.34 (2016): 9384-9387.</p>

<p>[This page is optimized for printing. Please consider saving paper by refraining from printing unless absolutely necessary.]</p>]]></content><author><name>Amitabh Yadav</name></author><category term="digital design and simulation" /><summary type="html"><![CDATA[Sara Hooker, “The hardware lottery”, Commun. ACM 64, 12, December 2021, pp. 58-65, https://doi.org/10.1145/3467017]]></summary></entry><entry><title type="html">Formal Verification primer</title><link href="http://localhost:4000/posts/2022/formal-verification-primer/" rel="alternate" type="text/html" title="Formal Verification primer" /><published>2022-05-05T00:00:00+02:00</published><updated>2022-05-05T00:00:00+02:00</updated><id>http://localhost:4000/posts/2022/formal-verification-primer</id><content type="html" xml:base="http://localhost:4000/posts/2022/formal-verification-primer/"><![CDATA[<blockquote>
  <p>Formal verification (abbr. as FV) is the use of tools that mathematically analyze the space of possible behaviors of a design, rather than computing results for particular values.</p>
</blockquote>

<p>This means that FV tools will look a the full space of possible simulations (by using clever mathematical techniques) rather than trying out specific values. Today, FV is an essential element of the design and tapeout flow, specially in critial areas, that can help prevent dangerous bug escapes of potentially serious repurcussions (for example, <a href="https://en.wikipedia.org/wiki/Pentium_FDIV_bug">Intel FDIV bug from early 1990s</a>).</p>

<p>FV should be employed from the start of early development up through the post silicon debug process appropriately at every design phase to improve design throughput, increase the confidence in the design and reduce the time to market.</p>

<p>There are essentially many reasons to inculcate formal verification in your design flow. Some of them are as follow:</p>

<h3 id="references">References:</h3>
<ol>
  <li>Eric Seligman, Tom Schubert, M V Achutha Kiran Kumar, <em>Formal Verification: An Essential Toolkit for Modern VLSI Design</em>, Morgan Kaufmann publications by Elsevier Inc. (2015).</li>
</ol>

<h3 id="disclaimer">Disclaimer:</h3>
<p>A large part of the post are direct paraphrasing or direct quotations from the books and other online references. All credits are therefore attributed to the original authors. This post serves as a means to organize and present the useful infomation from different sources in a concise and practical way.</p>

<p>[This page is optimized for printing. Please consider saving paper by refraining from printing unless absolutely necessary.]</p>]]></content><author><name>Amitabh Yadav</name></author><category term="digital design and simulation" /><summary type="html"><![CDATA[Formal verification (abbr. as FV) is the use of tools that mathematically analyze the space of possible behaviors of a design, rather than computing results for particular values. This means that FV tools will look a the full space of possible simulations (by using clever mathematical techniques) rather than trying out specific values. Today, FV is an essential element of the design and tapeout flow, specially in critial areas, that can help prevent dangerous bug escapes of potentially serious repurcussions (for example, Intel FDIV bug from early 1990s). FV should be employed from the start of early development up through the post silicon debug process appropriately at every design phase to improve design throughput, increase the confidence in the design and reduce the time to market. There are essentially many reasons to inculcate formal verification in your design flow. Some of them are as follow: References: Eric Seligman, Tom Schubert, M V Achutha Kiran Kumar, Formal Verification: An Essential Toolkit for Modern VLSI Design, Morgan Kaufmann publications by Elsevier Inc. (2015). Disclaimer: A large part of the post are direct paraphrasing or direct quotations from the books and other online references. All credits are therefore attributed to the original authors. This post serves as a means to organize and present the useful infomation from different sources in a concise and practical way. [This page is optimized for printing. Please consider saving paper by refraining from printing unless absolutely necessary.]]]></summary></entry><entry><title type="html">Tcl Scripting Tutorial for Vivado</title><link href="http://localhost:4000/posts/2022/tcl-scripting-in-vivado/" rel="alternate" type="text/html" title="Tcl Scripting Tutorial for Vivado" /><published>2022-05-05T00:00:00+02:00</published><updated>2022-05-05T00:00:00+02:00</updated><id>http://localhost:4000/posts/2022/tcl-scripting-in-vivado</id><content type="html" xml:base="http://localhost:4000/posts/2022/tcl-scripting-in-vivado/"><![CDATA[<p>Tcl is Tool Command Language! <em>lol! I really didn’t know that.</em></p>

<p>It’s an interpreted programming language with variables, procedures (procs), and control structures. It has been adopted as the standard application programming interface, or API, among most EDA vendors to control and extend their applications. The Tcl interpreter inside the Vivado Design Suite provides the full power and flexibility of Tcl to control the application, access design objects and their properties, and create custom reports.</p>

<p>Xilinx Design Constraints (XDC) to specify the design constraints. XDC is based on a subset of all the Tcl commands available in Vivado and is interpreted exactly like Tcl. The XDC commands are primarily timing constraints, physical constraints, object queries and a few Tcl built-in commands: <code class="language-plaintext highlighter-rouge">set</code>, <code class="language-plaintext highlighter-rouge">list</code>, and <code class="language-plaintext highlighter-rouge">expr</code>.</p>

<p>The Vivado tools write a journal file called <code class="language-plaintext highlighter-rouge">vivado.jou</code> into the directory from which Vivado was launched. The journal is a record of the Tcl commands run during the session that can be used as a starting point to create new Tcl scripts.</p>

<p>A log file, <code class="language-plaintext highlighter-rouge">vivado.log</code> is also created by the tool and includes the output of the commands that are executed. Both the journal and log file are useful to verify which commands were run and what result they produced.</p>

<p>[ This document is a work in progress until May 30, 2022. The content already published as correct and up to date. ]</p>

<h3 id="references">References:</h3>
<p>This tutorial is primarily based on:</p>
<ol>
  <li>Xilinx Document# <a href="https://docs.xilinx.com/r/2021.2-English/ug894-vivado-tcl-scripting/Tcl-Scripting-in-Vivado">UG894 (v2021.2) Using Tcl Scripting</a> and,</li>
  <li><a href="https://docs.xilinx.com/r/2021.2-English/ug835-vivado-tcl-commands/Introduction">UG835 (v2021.2) Vivado Design Suite Tcl Command Reference Guide</a>.</li>
</ol>]]></content><author><name>Amitabh Yadav</name></author><category term="digital design and simulation" /><summary type="html"><![CDATA[Tcl is Tool Command Language! lol! I really didn’t know that. It’s an interpreted programming language with variables, procedures (procs), and control structures. It has been adopted as the standard application programming interface, or API, among most EDA vendors to control and extend their applications. The Tcl interpreter inside the Vivado Design Suite provides the full power and flexibility of Tcl to control the application, access design objects and their properties, and create custom reports. Xilinx Design Constraints (XDC) to specify the design constraints. XDC is based on a subset of all the Tcl commands available in Vivado and is interpreted exactly like Tcl. The XDC commands are primarily timing constraints, physical constraints, object queries and a few Tcl built-in commands: set, list, and expr. The Vivado tools write a journal file called vivado.jou into the directory from which Vivado was launched. The journal is a record of the Tcl commands run during the session that can be used as a starting point to create new Tcl scripts. A log file, vivado.log is also created by the tool and includes the output of the commands that are executed. Both the journal and log file are useful to verify which commands were run and what result they produced. [ This document is a work in progress until May 30, 2022. The content already published as correct and up to date. ] References: This tutorial is primarily based on: Xilinx Document# UG894 (v2021.2) Using Tcl Scripting and, UG835 (v2021.2) Vivado Design Suite Tcl Command Reference Guide.]]></summary></entry><entry><title type="html">Computer Arithmetic : In Depth</title><link href="http://localhost:4000/posts/2022/computer-arithmetic-in-depth/" rel="alternate" type="text/html" title="Computer Arithmetic : In Depth" /><published>2022-04-22T00:00:00+02:00</published><updated>2022-04-22T00:00:00+02:00</updated><id>http://localhost:4000/posts/2022/computer-arithmetic</id><content type="html" xml:base="http://localhost:4000/posts/2022/computer-arithmetic-in-depth/"><![CDATA[<h2 id="number-representation">Number Representation</h2>

<p>Conventional and oftentimes exotic methods of number representation can lead to performance gains in terms of simpler and faster circuits. While conventional methods are used extensively, the unconventional ones find application in special purpose digital circuits and systems.</p>

<blockquote>
  <p>Computer arithmetic deals with the hardware realization of arithmetic functions to support various computer architectures as well as with arithmetic algorithms for firmware or software implementation.</p>
</blockquote>

<p>A major thrust of digital computer arithmetic is the design of hardware algorithms and circuits to enhance the speed of numeric operations. Thus computer arithmetic complements the architectural and algorithmic speedup techniques for high-performance computer architecture design and parallel processing.</p>

<p>Objectives:</p>

<ul>
  <li>To be able to describe the core design elements of the Arithmetic and Logic Unit (ALUs) of top-of-the-line CPUs designed using high-performance parallel arithmetic circuits.</li>
  <li>At times we will also deal with slow bit-serial designs for embedded applications, where implementation cost and input/output pin limitations are of prime concern.</li>
  <li>It would be a mistake, though, to conclude that computer arithmetic is useful only to computer designers. We will see shortly that you can use scientific calculations more effectively and write programs that are more accurate and/or more efficient after a study of computer arithmetic.</li>
  <li>You will be able to render informed judgment when faced with the problem of choosing a digital signal processor chip for your project.</li>
</ul>

<h3 id="fixed-point-numbers">Fixed Point Numbers</h3>

<p><img src="/images/computer-arithmetic/scope-of-computer-arithmetic.png" alt="The scope of computer arithmetic" /></p>

<h3 id="references">References:</h3>

<ol>
  <li>Behrooz Parhami, <em>Computer Arithmetic: Algorithms and Hardware Designs</em>, 2nd Edition, Oxford University Press, New York, 2010.</li>
  <li>Henry S. Warren, Jr. , <em>Hacker’s Delight</em>, 2nd Edition, Addison-Wesley Professional, September 2012.</li>
</ol>]]></content><author><name>Amitabh Yadav</name></author><category term="computer arithmetic" /><summary type="html"><![CDATA[Number Representation Conventional and oftentimes exotic methods of number representation can lead to performance gains in terms of simpler and faster circuits. While conventional methods are used extensively, the unconventional ones find application in special purpose digital circuits and systems. Computer arithmetic deals with the hardware realization of arithmetic functions to support various computer architectures as well as with arithmetic algorithms for firmware or software implementation. A major thrust of digital computer arithmetic is the design of hardware algorithms and circuits to enhance the speed of numeric operations. Thus computer arithmetic complements the architectural and algorithmic speedup techniques for high-performance computer architecture design and parallel processing. Objectives: To be able to describe the core design elements of the Arithmetic and Logic Unit (ALUs) of top-of-the-line CPUs designed using high-performance parallel arithmetic circuits. At times we will also deal with slow bit-serial designs for embedded applications, where implementation cost and input/output pin limitations are of prime concern. It would be a mistake, though, to conclude that computer arithmetic is useful only to computer designers. We will see shortly that you can use scientific calculations more effectively and write programs that are more accurate and/or more efficient after a study of computer arithmetic. You will be able to render informed judgment when faced with the problem of choosing a digital signal processor chip for your project. Fixed Point Numbers References: Behrooz Parhami, Computer Arithmetic: Algorithms and Hardware Designs, 2nd Edition, Oxford University Press, New York, 2010. Henry S. Warren, Jr. , Hacker’s Delight, 2nd Edition, Addison-Wesley Professional, September 2012.]]></summary></entry></feed>