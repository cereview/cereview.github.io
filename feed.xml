<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2021-06-26T11:30:00+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Amitabh’s Weblog</title><subtitle>C E R E V I E W</subtitle><author><name>Amitabh Yadav</name></author><entry><title type="html">Quantization and Training of NN for efficient Integer-Arithmetic-Only Inference</title><link href="http://localhost:4000/Quantum-Algorithms-for-Hamiltonian-Simulation/" rel="alternate" type="text/html" title="Quantization and Training of NN for efficient Integer-Arithmetic-Only Inference" /><published>2020-04-20T00:00:00+02:00</published><updated>2020-04-20T00:00:00+02:00</updated><id>http://localhost:4000/Quantum-Algorithms-for-Hamiltonian-Simulation</id><content type="html" xml:base="http://localhost:4000/Quantum-Algorithms-for-Hamiltonian-Simulation/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Modern CNN architectures have very high model complexity and demard high computational efficiency. Mobile devices, however, present the challenge to accomodate within limited memory and meeting low latency to maintina user engagement. The cost of performing inference on mobile devices like, smartphones, AR/VR devices, drones etc. bears large computation and memory overhead. In order to have more efficient inferece hardware on mobile devices with integer-only arithmetic hardware, a &lt;em&gt;quantization scheme&lt;/em&gt; helps in developing integer-only arithmetic to substitute the more sophesticated floating-point arithmetic hardware. Quantization, therefore, is the tradeoff between accuracy and on-device latency. The task at hand is to reduce model sizes and inference times, with minimal loss to accuracy. This also significantly affects the training procedure and requires the computer architect to maintain end-to-end model accuracy.&lt;/p&gt;

&lt;p&gt;This paper presents such a &lt;em&gt;quantization scheme&lt;/em&gt; along with a hardware/software co-design training procedure. The improvements are demonstracted on MobileNet (a model family for run-time efficiency) running ImageNet classification and COCO object detection on modern CPUs.&lt;/p&gt;

&lt;h4 id=&quot;contributions-of-the-paper&quot;&gt;Contributions of the paper:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;A &lt;em&gt;quantization scheme&lt;/em&gt; to quantize &lt;em&gt;weights&lt;/em&gt; and &lt;em&gt;activations&lt;/em&gt; as 8-bit integers, and other few parameters (such as, bias vectors) as 32-bit integers. This quantization scheme is derived from Ref.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; suggesting fixed-point arithemetic to accelerate training speed and Ref.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; suggesting 8-bit fixed-point arithemetic to accelerate inference on x86 architecture.&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;quantization inference framework&lt;/em&gt; for integer-arithmetic only hardware.&lt;/li&gt;
  &lt;li&gt;A co-designed &lt;em&gt;quantization training framework&lt;/em&gt; to maintain accuracy of inferece.&lt;/li&gt;
  &lt;li&gt;Presents the implementation of the frameworks on&lt;em&gt;MobileNet&lt;/em&gt;running on ARM CPUs, to perform classification (ImageNet&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;) and object detection (COCO &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;
&lt;p&gt;The quantization scheme employs an Integer-arithmetic only for Inference; and a Floating-point arithmetic for training. Both representation have a high degree of correlation with each other by separate adoption of the &lt;em&gt;quantization scheme&lt;/em&gt; for each.&lt;/p&gt;

&lt;p&gt;\(q \rightarrow\) &lt;em&gt;quantized value&lt;/em&gt;, denotes bit-representation of values.&lt;/p&gt;

&lt;p&gt;\(r \rightarrow\) &lt;em&gt;real value&lt;/em&gt;, denotes the actual numerical value.&lt;/p&gt;

&lt;p&gt;The integer value to quantized value &lt;em&gt;mapping&lt;/em&gt; is given as follows:&lt;/p&gt;

\[r=S(q-Z)\]

&lt;p&gt;where, \(S\) and \(Z\) are some constants called &lt;em&gt;quantization parameters&lt;/em&gt;. q can be quantized as &lt;em&gt;B&lt;/em&gt;-bit integer for &lt;em&gt;B&lt;/em&gt;-bit quantization. Here, &lt;em&gt;B&lt;/em&gt; is 8-bits. Bias vectors are quantized as 32-bit integers.&lt;/p&gt;

&lt;p&gt;Here, a single set of quantization parameters is used for both &lt;em&gt;weights array&lt;/em&gt; and &lt;em&gt;activations array&lt;/em&gt;. Separate arrays can use separate quantization parameters. The mapping can be implemented as SIMD as opposed to the alternate method of using look-up table, to have better performance.&lt;/p&gt;

&lt;h4 id=&quot;8-bit-quantization&quot;&gt;8-bit Quantization&lt;/h4&gt;
&lt;p&gt;\(r=S(q-Z)\)&lt;/p&gt;

&lt;p&gt;\(S \rightarrow\) &lt;em&gt;“Scale”&lt;/em&gt; is arbitrary positive number. In software, it is a floating-point number just like the real value \(r\), of type &lt;code class=&quot;highlighter-rouge&quot;&gt;float&lt;/code&gt;. Note that for inference, the floating-point quantities need to be eliminated (discussed ahead).&lt;/p&gt;

&lt;p&gt;\(Z \rightarrow\) &lt;em&gt;“Zero-point”&lt;/em&gt; is the &lt;em&gt;quantized value&lt;/em&gt; corresponding to \(0\), and is of the same type as \(q\) i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;uint8&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Using the above mapping, 0 is exactly representable in real value as well. NN implementation often has 0-padding of arrays around boundaries. This property of the mapping facilitates in that.&lt;/p&gt;

&lt;h4 id=&quot;how-to-do-integer-arithmetic-only-matrix-multiplication&quot;&gt;How to do Integer-Arithmetic-Only Matrix Multiplication?&lt;/h4&gt;
&lt;p&gt;Currently, \(r\) and \(S\) are floating point, and we need to have an integer-arithmetic-only inference scheme using the mapping \(r=S(q-Z)\).&lt;/p&gt;

&lt;p&gt;Given, \(N \times N\) matrix composed of real values \(r_1\) and \(r_2\). Their product matrix has values \(r_3 = r_1r_2\). We have \(r_{\alpha}^{i,j}\) such that, \(1 \leq i,j \leq N\), with quantization parameters \((S_\alpha, Z_\alpha)\).&lt;/p&gt;

\[r_{\alpha}^{i,j} = S_\alpha(q_{\alpha}^{i,j} - Z_\alpha)\]

&lt;p&gt;By performing matrix multiplication, we have:&lt;/p&gt;

\[S_3(q_{3}^{i,k} - Z_3) = \sum_{j=1}^N S_1(q_{1}^{i,j} - Z_1)S_2(q_{2}^{j,k} - Z_2)\]

&lt;p&gt;rewriting,&lt;/p&gt;

\[q_{3}^{i,k} = Z_3 + M\sum_{j=1}^N (q_{1}^{i,j} - Z_1)(q_{2}^{j,k} - Z_2)\]

&lt;p&gt;where \(M\) remains the only non-integer constant which can be calculated offline using quantization scales \(S_1, S_2\) and \(S_3\),&lt;/p&gt;

\[M := \frac{S_1S_2}{S_3}\]

&lt;p&gt;Empirically, it is determined that 0 \lt M \lt 1, and therefore can be expressed in the normalized form as follows:&lt;/p&gt;

\[M = 2^{-n}M_0\]

&lt;p&gt;where, \(M_0 \in (0.5,1]\) and \(n \in \mathbb I^{+}\). The normalized \(M_0\) can be expressed as a fixed-point multiplier e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;int16&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;int32&lt;/code&gt; depending on hardware. E.g. if &lt;code class=&quot;highlighter-rouge&quot;&gt;int32&lt;/code&gt; is used the \(M_0\) will be the value closest to \(2^{31}M_0\). Also, since \(M_0 \gt 0.5\), so \(M_0\) has atleast 30bits of relative accuracy.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; This way, multiplication operation with \(M_0\) can be expressed as fixed point multiplication and the multiplication with \(2^{-n}\) can be done by bit-shifts with correct round-to-nearest behaviour.&lt;/p&gt;

&lt;h4 id=&quot;efficient-handing-of-zero-points&quot;&gt;Efficient Handing of Zero-points&lt;/h4&gt;

&lt;h4 id=&quot;implemention-of-typical-fused-layer&quot;&gt;Implemention of typical fused layer&lt;/h4&gt;

&lt;h3 id=&quot;training-with-simulated-quantization&quot;&gt;Training with Simulated Quantization&lt;/h3&gt;

&lt;h3 id=&quot;experiment-and-results&quot;&gt;Experiment and Results&lt;/h3&gt;

&lt;h2 id=&quot;analysis-strengths-and-weaknesses&quot;&gt;Analysis: Strengths and Weaknesses&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;This paper presents a detailed approach to reducing the inference latency on devices with limited memory and compute power. Unlike using an improper baseline architecture such as AlexNet, GoogleNet etc. to compare which does not reflect the advantage for mobile devices, this paper presents the improvement on MobileNet architectures, which makes the results significantly important for the Computer Architecture community.&lt;/li&gt;
  &lt;li&gt;Another important strength of the paper is that if presents the co-design framework and the experimental implementation on prominent mobile CPU for both training and inference using the proposed methodology i.e. the &lt;em&gt;quantization scheme&lt;/em&gt;. The benchmarking is done using ImageNet and COCO, which present a useful comparison of accuracy between the tradition FP-hardware and Integer-arithmetic hardware.&lt;/li&gt;
  &lt;li&gt;The weakness of the paper is that, it does not discuss about efficient integer-arithmetic representation for varying weight/activation vector sizes. Such as, why was 8-bit for weights and activation chosen, what would be the effect of chosing a higher bit counts, why does bias get 32-bit etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ideas-and-future-directions&quot;&gt;Ideas and Future Directions&lt;/h2&gt;

&lt;h2 id=&quot;lessons-learned-and-questions&quot;&gt;Lessons learned and Questions?&lt;/h2&gt;
&lt;p&gt;The only question I’m left pondering is wheter a mathematical model can be derived for comparing the FP vs Integer hardware? i.e. a theoretical comparison of accuracy of FP arithemetic approach against the expected deterioration of the proposed approach, and how much error cane be accomodated without compensating for the overall accuracy of the inference.&lt;/p&gt;

&lt;h2 id=&quot;some-other-interesting-approaches&quot;&gt;Some other interesting approaches&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H. and Kalenichenko, D., 2018. &lt;em&gt;“Quantization and training of neural networks for efficient integer-arithmetic-only inference”&lt;/em&gt;. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2704-2713).&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Gupta, Suyog, et al. “Deep learning with limited numerical precision.” International Conference on Machine Learning. 2015. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao. “Improving the speed of neural networks on CPUs.” (2011). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Deng, Jia, et al. “Imagenet: A large-scale hierarchical image database.” 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lin, Tsung-Yi, et al. “Microsoft coco: Common objects in context.” European conference on computer vision. Springer, Cham, 2014. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Explaination Required. Further explained in Appendix B. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Amitabh Yadav</name></author><category term="Machine Learning Hardware" /><category term="machine learning" /><category term="convolutional neural networks" /><category term="machine learning quantization" /><summary type="html">Summary Modern CNN architectures have very high model complexity and demard high computational efficiency. Mobile devices, however, present the challenge to accomodate within limited memory and meeting low latency to maintina user engagement. The cost of performing inference on mobile devices like, smartphones, AR/VR devices, drones etc. bears large computation and memory overhead. In order to have more efficient inferece hardware on mobile devices with integer-only arithmetic hardware, a quantization scheme helps in developing integer-only arithmetic to substitute the more sophesticated floating-point arithmetic hardware. Quantization, therefore, is the tradeoff between accuracy and on-device latency. The task at hand is to reduce model sizes and inference times, with minimal loss to accuracy. This also significantly affects the training procedure and requires the computer architect to maintain end-to-end model accuracy. This paper presents such a quantization scheme along with a hardware/software co-design training procedure. The improvements are demonstracted on MobileNet (a model family for run-time efficiency) running ImageNet classification and COCO object detection on modern CPUs. Contributions of the paper: A quantization scheme to quantize weights and activations as 8-bit integers, and other few parameters (such as, bias vectors) as 32-bit integers. This quantization scheme is derived from Ref.1 suggesting fixed-point arithemetic to accelerate training speed and Ref.2 suggesting 8-bit fixed-point arithemetic to accelerate inference on x86 architecture. A quantization inference framework for integer-arithmetic only hardware. A co-designed quantization training framework to maintain accuracy of inferece. Presents the implementation of the frameworks onMobileNetrunning on ARM CPUs, to perform classification (ImageNet3) and object detection (COCO 4). Details The quantization scheme employs an Integer-arithmetic only for Inference; and a Floating-point arithmetic for training. Both representation have a high degree of correlation with each other by separate adoption of the quantization scheme for each. \(q \rightarrow\) quantized value, denotes bit-representation of values. \(r \rightarrow\) real value, denotes the actual numerical value. The integer value to quantized value mapping is given as follows: \[r=S(q-Z)\] where, \(S\) and \(Z\) are some constants called quantization parameters. q can be quantized as B-bit integer for B-bit quantization. Here, B is 8-bits. Bias vectors are quantized as 32-bit integers. Here, a single set of quantization parameters is used for both weights array and activations array. Separate arrays can use separate quantization parameters. The mapping can be implemented as SIMD as opposed to the alternate method of using look-up table, to have better performance. 8-bit Quantization \(r=S(q-Z)\) \(S \rightarrow\) “Scale” is arbitrary positive number. In software, it is a floating-point number just like the real value \(r\), of type float. Note that for inference, the floating-point quantities need to be eliminated (discussed ahead). \(Z \rightarrow\) “Zero-point” is the quantized value corresponding to \(0\), and is of the same type as \(q\) i.e. uint8. Using the above mapping, 0 is exactly representable in real value as well. NN implementation often has 0-padding of arrays around boundaries. This property of the mapping facilitates in that. How to do Integer-Arithmetic-Only Matrix Multiplication? Currently, \(r\) and \(S\) are floating point, and we need to have an integer-arithmetic-only inference scheme using the mapping \(r=S(q-Z)\). Given, \(N \times N\) matrix composed of real values \(r_1\) and \(r_2\). Their product matrix has values \(r_3 = r_1r_2\). We have \(r_{\alpha}^{i,j}\) such that, \(1 \leq i,j \leq N\), with quantization parameters \((S_\alpha, Z_\alpha)\). \[r_{\alpha}^{i,j} = S_\alpha(q_{\alpha}^{i,j} - Z_\alpha)\] By performing matrix multiplication, we have: \[S_3(q_{3}^{i,k} - Z_3) = \sum_{j=1}^N S_1(q_{1}^{i,j} - Z_1)S_2(q_{2}^{j,k} - Z_2)\] rewriting, \[q_{3}^{i,k} = Z_3 + M\sum_{j=1}^N (q_{1}^{i,j} - Z_1)(q_{2}^{j,k} - Z_2)\] where \(M\) remains the only non-integer constant which can be calculated offline using quantization scales \(S_1, S_2\) and \(S_3\), \[M := \frac{S_1S_2}{S_3}\] Empirically, it is determined that 0 \lt M \lt 1, and therefore can be expressed in the normalized form as follows: \[M = 2^{-n}M_0\] where, \(M_0 \in (0.5,1]\) and \(n \in \mathbb I^{+}\). The normalized \(M_0\) can be expressed as a fixed-point multiplier e.g. int16 or int32 depending on hardware. E.g. if int32 is used the \(M_0\) will be the value closest to \(2^{31}M_0\). Also, since \(M_0 \gt 0.5\), so \(M_0\) has atleast 30bits of relative accuracy.5 This way, multiplication operation with \(M_0\) can be expressed as fixed point multiplication and the multiplication with \(2^{-n}\) can be done by bit-shifts with correct round-to-nearest behaviour. Efficient Handing of Zero-points Implemention of typical fused layer Training with Simulated Quantization Experiment and Results Analysis: Strengths and Weaknesses This paper presents a detailed approach to reducing the inference latency on devices with limited memory and compute power. Unlike using an improper baseline architecture such as AlexNet, GoogleNet etc. to compare which does not reflect the advantage for mobile devices, this paper presents the improvement on MobileNet architectures, which makes the results significantly important for the Computer Architecture community. Another important strength of the paper is that if presents the co-design framework and the experimental implementation on prominent mobile CPU for both training and inference using the proposed methodology i.e. the quantization scheme. The benchmarking is done using ImageNet and COCO, which present a useful comparison of accuracy between the tradition FP-hardware and Integer-arithmetic hardware. The weakness of the paper is that, it does not discuss about efficient integer-arithmetic representation for varying weight/activation vector sizes. Such as, why was 8-bit for weights and activation chosen, what would be the effect of chosing a higher bit counts, why does bias get 32-bit etc. Ideas and Future Directions Lessons learned and Questions? The only question I’m left pondering is wheter a mathematical model can be derived for comparing the FP vs Integer hardware? i.e. a theoretical comparison of accuracy of FP arithemetic approach against the expected deterioration of the proposed approach, and how much error cane be accomodated without compensating for the overall accuracy of the inference. Some other interesting approaches Gupta, Suyog, et al. “Deep learning with limited numerical precision.” International Conference on Machine Learning. 2015. &amp;#8617; Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao. “Improving the speed of neural networks on CPUs.” (2011). &amp;#8617; Deng, Jia, et al. “Imagenet: A large-scale hierarchical image database.” 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009. &amp;#8617; Lin, Tsung-Yi, et al. “Microsoft coco: Common objects in context.” European conference on computer vision. Springer, Cham, 2014. &amp;#8617; Explaination Required. Further explained in Appendix B. &amp;#8617;</summary></entry><entry><title type="html">cuDNN: Efficient Primitives for Deep Learning</title><link href="http://localhost:4000/cuDNN-Efficient-Primitives-for-Deep-Learning/" rel="alternate" type="text/html" title="cuDNN: Efficient Primitives for Deep Learning" /><published>2020-04-18T00:00:00+02:00</published><updated>2020-04-18T00:00:00+02:00</updated><id>http://localhost:4000/cuDNN-Efficient-Primitives-for-Deep-Learning</id><content type="html" xml:base="http://localhost:4000/cuDNN-Efficient-Primitives-for-Deep-Learning/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;h4 id=&quot;contributions-of-the-paper&quot;&gt;Contributions of the paper:&lt;/h4&gt;

&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;

&lt;h3 id=&quot;training-with-simulated-quantization&quot;&gt;Training with Simulated Quantization&lt;/h3&gt;

&lt;h3 id=&quot;experiment-and-results&quot;&gt;Experiment and Results&lt;/h3&gt;

&lt;h2 id=&quot;analysis-strengths-and-weaknesses&quot;&gt;Analysis: Strengths and Weaknesses&lt;/h2&gt;

&lt;h2 id=&quot;ideas-and-future-directions&quot;&gt;Ideas and Future Directions&lt;/h2&gt;

&lt;h2 id=&quot;lessons-learned-and-questions&quot;&gt;Lessons learned and Questions?&lt;/h2&gt;

&lt;h2 id=&quot;some-other-interesting-approaches&quot;&gt;Some other interesting approaches&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Chetlur, Sharan, et al. “cudnn: Efficient primitives for deep learning.” arXiv preprint arXiv:1410.0759 (2014).&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="Machine Learning Hardware" /><category term="machine learning" /><category term="convolutional neural networks" /><category term="cuDNN" /><category term="deep learning library" /><summary type="html">Summary</summary></entry><entry><title type="html">Course Notes: EE290-2 Hardware For Machine Learning</title><link href="http://localhost:4000/Course-Notes-ee290-2-hardware-for-machine-learning-ucb/" rel="alternate" type="text/html" title="Course Notes: EE290-2 Hardware For Machine Learning" /><published>2020-04-15T00:00:00+02:00</published><updated>2020-04-15T00:00:00+02:00</updated><id>http://localhost:4000/Course-Notes-ee290-2-hardware-for-machine-learning-ucb</id><content type="html" xml:base="http://localhost:4000/Course-Notes-ee290-2-hardware-for-machine-learning-ucb/">&lt;p&gt;Notes for the UC Berkeley Course, EE290-2 Hardware for Machine Learning, Spring 2020.&lt;/p&gt;

&lt;p&gt;Lecturer: Prof. Dr. Sophia Shao&lt;/p&gt;

&lt;p&gt;Main topics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deep Neural Networks&lt;/li&gt;
  &lt;li&gt;Quantization&lt;/li&gt;
  &lt;li&gt;Development Platforms&lt;/li&gt;
  &lt;li&gt;Kernel Computations
-Dataflows&lt;/li&gt;
  &lt;li&gt;State of the art accelerators&lt;/li&gt;
  &lt;li&gt;Mapping&lt;/li&gt;
  &lt;li&gt;Sparsity&lt;/li&gt;
  &lt;li&gt;Hardware/Software Co-Design&lt;/li&gt;
  &lt;li&gt;Other NN architectures: RNN, NLP, RM, RL etc&lt;/li&gt;
  &lt;li&gt;Advanced Technology&lt;/li&gt;
  &lt;li&gt;Training&lt;/li&gt;
  &lt;li&gt;ML for Hardware Design, Power Estimation etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes-lecture-1&quot;&gt;Notes: Lecture 1&lt;/h2&gt;
&lt;p&gt;The goal is to make efficient hardware for Machine Learning and Deep Learning applications. The idea is that, we can design massively parallel architecture with hardware level optimizations by exploiting the knowledge of key machine learning algorithm characteristics.&lt;/p&gt;

&lt;h4 id=&quot;introduction-to-deep-learning&quot;&gt;Introduction to Deep Learning&lt;/h4&gt;
&lt;p&gt;Deep Learning models have grown vastly in size and require large amounts of compute power.  Such DL kernels run on every compute device such as, Mobile Devices, Drones/Robots, Workstations, Data Centers etc.&lt;/p&gt;

&lt;p&gt;The figure below from &lt;a href=&quot;https://openai.com/blog/ai-and-compute/&quot;&gt;OpenAI&lt;/a&gt; shows the  exponential increase of compute for AI training. This has been largely attributed to modern hardware development.
&lt;img src=&quot;../images/ee290-2-01.png&quot; alt=&quot;OpenAi Compute&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How hardware intense this could get, as shown in &lt;a href=&quot;https://arxiv.org/abs/1711.04325.pdf&quot;&gt;this paper&lt;/a&gt;, is as follows:
&lt;img src=&quot;../images/ee290-2-02.png&quot; alt=&quot;Training ResNet50&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ques: What is 90epoch?&lt;/strong&gt; &lt;em&gt;answer goes here.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Moore’s Law is destined to cease in the coming years as the technology size has shrunk down to &lt;a href=&quot;https://www.tsmc.com/english/dedicatedFoundry/technology/5nm.htm&quot;&gt;5nm&lt;/a&gt; so that the gate lengths get as thin as a few layers of atoms. Domain specific accelrators and a vertical integration approach would be the dominant way forward to achieve application speedups and meet the high compute requirements of  Deep Learning Kernels. The following figure shows the teardown of IPhoneXS and the number of specialized kernels. The number of specialized IP blocks such as the Domain Specific Accelerators, Neural Engine etc has grown nearly in a linear fashion &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/7106399&quot;&gt;Ref: Shao et.al. IEEE Micro 2015&lt;/a&gt; from abut 9 in the ARM A4 processor in 2010 to about 40 in the ARM A11 in 2017.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ee290-2-03.png&quot; alt=&quot;Apple iPhoneXS Teardown&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Deep Learning Rigs:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;NVIDIA Volta V100 GPU:
    &lt;ul&gt;
      &lt;li&gt;21B Transistors at TSMC 12nm FinFET.&lt;/li&gt;
      &lt;li&gt;15.7 TFLOPS/s Single Precision-FP (FP32) performance.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
      &lt;li&gt;125 Tensor TFLOP/s of mixed-precision Matrix-Multipl-and-Accumulate.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
      &lt;li&gt;300W&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NVIDIA DGX-2 = 16 \(\times\) Tesla V100.
    &lt;ul&gt;
      &lt;li&gt;Power: 10kW&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cerebras: Wafer-Scale Deep Learning
    &lt;ul&gt;
      &lt;li&gt;1.2 Trillion Transistors&lt;/li&gt;
      &lt;li&gt;46,225 mm\(^2\) silicon die area.&lt;/li&gt;
      &lt;li&gt;400,000 Optimized Cores&lt;/li&gt;
      &lt;li&gt;16GB Onchip Memory&lt;/li&gt;
      &lt;li&gt;TSMC 16nm&lt;/li&gt;
      &lt;li&gt;Challenges:
        &lt;ul&gt;
          &lt;li&gt;Process Variations&lt;/li&gt;
          &lt;li&gt;Heating&lt;/li&gt;
          &lt;li&gt;Routing/Interconnects, Second-order/higher-order effects etc.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tesla Self-Driving Computer
    &lt;ul&gt;
      &lt;li&gt;96\(\times\)96 MACs&lt;/li&gt;
      &lt;li&gt;32MB SRAM/instance&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Google TPU&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;how-to-compare-performance-of-hardware&quot;&gt;How to Compare Performance of Hardware?&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://mlperf.org/&quot;&gt;MLPerf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;More on this in later lectures&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;notes-lecture-2&quot;&gt;Notes: Lecture 2&lt;/h2&gt;
&lt;p&gt;AI: The science and engineering of creating intelligent machines. - John McCarthy, 1956.&lt;/p&gt;

&lt;p&gt;ML: The field of study that gives the computers the ability to learn without being explicitly programmed. - Arthur Samuel, 1959.&lt;/p&gt;

&lt;p&gt;DL: Seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels. - Yoshua Bengio, 2012.&lt;/p&gt;

&lt;p&gt;Three waves of NN came before present day scenario: Cybernetics (1940s-60s), Connectionism(1980s-90s) and Deep Learning(2006 - present). The size of dataset increased dramatically which made neural networks possible. Also hardware caught up and we could get better performance from the NN architectures. For example, CIFAR10 has 60,000 of 32\(\times\)32 images: 50k training+10k Test, arranged in 10 classes (eg. airplance, automobile, cat, bird etc) - 6000 images per class. The following figure shows dataset sizes from Goodfellow et.al.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ee290-2-04.png&quot; alt=&quot;Dataset Sizes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Increasing data-sizes implies Increasing Model Sizes as well. Model Size increase implies increase in:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Number of Neurons&lt;/li&gt;
  &lt;li&gt;Number of connections per neuron.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is largely do to the availability of better hardware and this trend, it seems, would continue in the forseeable future. The following figure shows the increasing model sizes based on aforementioned criteria.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/ee290-2-05.png&quot; alt=&quot;Model size increase trend&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;basics-of-ml-dataset-and-optimization-methods&quot;&gt;Basics of ML: Dataset and Optimization methods&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;“A computer program is said to learn from experience (E) with respect to some task (T) and some performance measure (P), if its
performance on T, as measured by P, improves with experience E.” - Tom Mitchell, 1998.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For example: Spam e-mail classification:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Task (T): Predict emails as spam or not spam.&lt;/li&gt;
  &lt;li&gt;Experience (E): Observe users label emails as spam or not.&lt;/li&gt;
  &lt;li&gt;Performance (P): Number of emails that are correctly predicted.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ML Algorithms are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Supervised: Data + labels given. e.g. Classification and Regression.&lt;/li&gt;
  &lt;li&gt;Unsupervised: Data only. No labels. e.g. Clustering&lt;/li&gt;
  &lt;li&gt;Reinforcement Learning: Interact with environment and learn.&lt;/li&gt;
  &lt;li&gt;Self-Supervised Learning: Contact:- Yann LeCun. ;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In nearly all ML algorithms, we can identify T, P and E as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset \(\rightarrow\) Experience(E)&lt;/li&gt;
  &lt;li&gt;Cost (loss) function \(\rightarrow\) Performance Measure (P)&lt;/li&gt;
  &lt;li&gt;Model + Optimization Method \(\rightarrow\) Task (T)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;example-linear-regression&quot;&gt;Example: Linear Regression&lt;/h5&gt;
&lt;p&gt;Linear Regression is supervised learning from a dataset to estimate a value (that may or may not be in the original dataset) based on given datavalues. E.g. find the real-estate sale price (US Dollars) in the given area, given dataset of sale values of different plot sizes in a particular area.&lt;/p&gt;

&lt;p&gt;How it works? We have data with Labels.&lt;/p&gt;

&lt;p&gt;Partition the data in two: (1) Training Set (to train the model), and (2) Test Set (to test the trained model) and (3) Validation Set.&lt;/p&gt;

&lt;p&gt;An overfitting or underfitting of model renders the model inefficient and returns wrong results.&lt;/p&gt;

&lt;p&gt;More detailed notes on Linear Regression is available in another blogpost.&lt;/p&gt;

&lt;h4 id=&quot;basics-of-deep-learning&quot;&gt;Basics of Deep Learning&lt;/h4&gt;

&lt;h4 id=&quot;alexnet&quot;&gt;AlexNet&lt;/h4&gt;
&lt;p&gt;We have discussed AlexNet in detail in the review post, &lt;a href=&quot;../ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-AlexNet/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;lecture-3-quantization&quot;&gt;Lecture 3: Quantization&lt;/h2&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;some-other-interesting-stuff&quot;&gt;Some other interesting Stuff:&lt;/h2&gt;
&lt;p&gt;Tips for writing paper reviews:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Motivation for the work.&lt;/li&gt;
  &lt;li&gt;Proposed Solution.&lt;/li&gt;
  &lt;li&gt;Evaluation of the proposed solution.&lt;/li&gt;
  &lt;li&gt;Analysis of identified problem, proposed solution and evaluation.&lt;/li&gt;
  &lt;li&gt;Future Directions for the research.&lt;/li&gt;
  &lt;li&gt;Queries.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;What is TFLOP/s? &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;What is Mixed Precision Matrix-Multipl-and-Accumulate? &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Amitabh Yadav</name></author><category term="Machine Learning Hardware" /><category term="machine learning" /><category term="uc berkeley eecs" /><category term="hardware for machine learning" /><summary type="html">Notes for the UC Berkeley Course, EE290-2 Hardware for Machine Learning, Spring 2020. Lecturer: Prof. Dr. Sophia Shao Main topics: Deep Neural Networks Quantization Development Platforms Kernel Computations -Dataflows State of the art accelerators Mapping Sparsity Hardware/Software Co-Design Other NN architectures: RNN, NLP, RM, RL etc Advanced Technology Training ML for Hardware Design, Power Estimation etc. Notes: Lecture 1 The goal is to make efficient hardware for Machine Learning and Deep Learning applications. The idea is that, we can design massively parallel architecture with hardware level optimizations by exploiting the knowledge of key machine learning algorithm characteristics. Introduction to Deep Learning Deep Learning models have grown vastly in size and require large amounts of compute power. Such DL kernels run on every compute device such as, Mobile Devices, Drones/Robots, Workstations, Data Centers etc. The figure below from OpenAI shows the exponential increase of compute for AI training. This has been largely attributed to modern hardware development. How hardware intense this could get, as shown in this paper, is as follows: Ques: What is 90epoch? answer goes here. Moore’s Law is destined to cease in the coming years as the technology size has shrunk down to 5nm so that the gate lengths get as thin as a few layers of atoms. Domain specific accelrators and a vertical integration approach would be the dominant way forward to achieve application speedups and meet the high compute requirements of Deep Learning Kernels. The following figure shows the teardown of IPhoneXS and the number of specialized kernels. The number of specialized IP blocks such as the Domain Specific Accelerators, Neural Engine etc has grown nearly in a linear fashion Ref: Shao et.al. IEEE Micro 2015 from abut 9 in the ARM A4 processor in 2010 to about 40 in the ARM A11 in 2017. Deep Learning Rigs: NVIDIA Volta V100 GPU: 21B Transistors at TSMC 12nm FinFET. 15.7 TFLOPS/s Single Precision-FP (FP32) performance.1 125 Tensor TFLOP/s of mixed-precision Matrix-Multipl-and-Accumulate.2 300W NVIDIA DGX-2 = 16 \(\times\) Tesla V100. Power: 10kW Cerebras: Wafer-Scale Deep Learning 1.2 Trillion Transistors 46,225 mm\(^2\) silicon die area. 400,000 Optimized Cores 16GB Onchip Memory TSMC 16nm Challenges: Process Variations Heating Routing/Interconnects, Second-order/higher-order effects etc. Tesla Self-Driving Computer 96\(\times\)96 MACs 32MB SRAM/instance Google TPU How to Compare Performance of Hardware? MLPerf More on this in later lectures Notes: Lecture 2 AI: The science and engineering of creating intelligent machines. - John McCarthy, 1956. ML: The field of study that gives the computers the ability to learn without being explicitly programmed. - Arthur Samuel, 1959. DL: Seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels. - Yoshua Bengio, 2012. Three waves of NN came before present day scenario: Cybernetics (1940s-60s), Connectionism(1980s-90s) and Deep Learning(2006 - present). The size of dataset increased dramatically which made neural networks possible. Also hardware caught up and we could get better performance from the NN architectures. For example, CIFAR10 has 60,000 of 32\(\times\)32 images: 50k training+10k Test, arranged in 10 classes (eg. airplance, automobile, cat, bird etc) - 6000 images per class. The following figure shows dataset sizes from Goodfellow et.al. Increasing data-sizes implies Increasing Model Sizes as well. Model Size increase implies increase in: Number of Neurons Number of connections per neuron. This is largely do to the availability of better hardware and this trend, it seems, would continue in the forseeable future. The following figure shows the increasing model sizes based on aforementioned criteria. Basics of ML: Dataset and Optimization methods “A computer program is said to learn from experience (E) with respect to some task (T) and some performance measure (P), if its performance on T, as measured by P, improves with experience E.” - Tom Mitchell, 1998. For example: Spam e-mail classification: Task (T): Predict emails as spam or not spam. Experience (E): Observe users label emails as spam or not. Performance (P): Number of emails that are correctly predicted. ML Algorithms are: Supervised: Data + labels given. e.g. Classification and Regression. Unsupervised: Data only. No labels. e.g. Clustering Reinforcement Learning: Interact with environment and learn. Self-Supervised Learning: Contact:- Yann LeCun. ;) In nearly all ML algorithms, we can identify T, P and E as follows: Dataset \(\rightarrow\) Experience(E) Cost (loss) function \(\rightarrow\) Performance Measure (P) Model + Optimization Method \(\rightarrow\) Task (T) Example: Linear Regression Linear Regression is supervised learning from a dataset to estimate a value (that may or may not be in the original dataset) based on given datavalues. E.g. find the real-estate sale price (US Dollars) in the given area, given dataset of sale values of different plot sizes in a particular area. How it works? We have data with Labels. Partition the data in two: (1) Training Set (to train the model), and (2) Test Set (to test the trained model) and (3) Validation Set. An overfitting or underfitting of model renders the model inefficient and returns wrong results. More detailed notes on Linear Regression is available in another blogpost. Basics of Deep Learning What is TFLOP/s? &amp;#8617; What is Mixed Precision Matrix-Multipl-and-Accumulate? &amp;#8617;</summary></entry><entry><title type="html">Quantization and Training of NN for efficient Integer-Arithmetic-Only Inference</title><link href="http://localhost:4000/Quantization-and-Training-of-Neural-Networks-for-efficient-Integer-Arithmetic-Only-Inference/" rel="alternate" type="text/html" title="Quantization and Training of NN for efficient Integer-Arithmetic-Only Inference" /><published>2020-04-14T00:00:00+02:00</published><updated>2020-04-14T00:00:00+02:00</updated><id>http://localhost:4000/Quantization-and-Training-of-Neural-Networks-for-efficient-Integer-Arithmetic-Only-Inference</id><content type="html" xml:base="http://localhost:4000/Quantization-and-Training-of-Neural-Networks-for-efficient-Integer-Arithmetic-Only-Inference/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Modern CNN architectures have very high model complexity and demard high computational efficiency. Mobile devices, however, present the challenge to accomodate within limited memory and meeting low latency to maintina user engagement. The cost of performing inference on mobile devices like, smartphones, AR/VR devices, drones etc. bears large computation and memory overhead. In order to have more efficient inferece hardware on mobile devices with integer-only arithmetic hardware, a &lt;em&gt;quantization scheme&lt;/em&gt; helps in developing integer-only arithmetic to substitute the more sophesticated floating-point arithmetic hardware. Quantization, therefore, is the tradeoff between accuracy and on-device latency. The task at hand is to reduce model sizes and inference times, with minimal loss to accuracy. This also significantly affects the training procedure and requires the computer architect to maintain end-to-end model accuracy.&lt;/p&gt;

&lt;p&gt;This paper presents such a &lt;em&gt;quantization scheme&lt;/em&gt; along with a hardware/software co-design training procedure. The improvements are demonstracted on MobileNet (a model family for run-time efficiency) running ImageNet classification and COCO object detection on modern CPUs.&lt;/p&gt;

&lt;h4 id=&quot;contributions-of-the-paper&quot;&gt;Contributions of the paper:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;A &lt;em&gt;quantization scheme&lt;/em&gt; to quantize &lt;em&gt;weights&lt;/em&gt; and &lt;em&gt;activations&lt;/em&gt; as 8-bit integers, and other few parameters (such as, bias vectors) as 32-bit integers. This quantization scheme is derived from Ref.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; suggesting fixed-point arithemetic to accelerate training speed and Ref.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; suggesting 8-bit fixed-point arithemetic to accelerate inference on x86 architecture.&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;quantization inference framework&lt;/em&gt; for integer-arithmetic only hardware.&lt;/li&gt;
  &lt;li&gt;A co-designed &lt;em&gt;quantization training framework&lt;/em&gt; to maintain accuracy of inferece.&lt;/li&gt;
  &lt;li&gt;Presents the implementation of the frameworks on&lt;em&gt;MobileNet&lt;/em&gt;running on ARM CPUs, to perform classification (ImageNet&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;) and object detection (COCO &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;
&lt;p&gt;The quantization scheme employs an Integer-arithmetic only for Inference; and a Floating-point arithmetic for training. Both representation have a high degree of correlation with each other by separate adoption of the &lt;em&gt;quantization scheme&lt;/em&gt; for each.&lt;/p&gt;

&lt;p&gt;\(q \rightarrow\) &lt;em&gt;quantized value&lt;/em&gt;, denotes bit-representation of values.&lt;/p&gt;

&lt;p&gt;\(r \rightarrow\) &lt;em&gt;real value&lt;/em&gt;, denotes the actual numerical value.&lt;/p&gt;

&lt;p&gt;The integer value to quantized value &lt;em&gt;mapping&lt;/em&gt; is given as follows:&lt;/p&gt;

\[r=S(q-Z)\]

&lt;p&gt;where, \(S\) and \(Z\) are some constants called &lt;em&gt;quantization parameters&lt;/em&gt;. q can be quantized as &lt;em&gt;B&lt;/em&gt;-bit integer for &lt;em&gt;B&lt;/em&gt;-bit quantization. Here, &lt;em&gt;B&lt;/em&gt; is 8-bits. Bias vectors are quantized as 32-bit integers.&lt;/p&gt;

&lt;p&gt;Here, a single set of quantization parameters is used for both &lt;em&gt;weights array&lt;/em&gt; and &lt;em&gt;activations array&lt;/em&gt;. Separate arrays can use separate quantization parameters. The mapping can be implemented as SIMD as opposed to the alternate method of using look-up table, to have better performance.&lt;/p&gt;

&lt;h4 id=&quot;8-bit-quantization&quot;&gt;8-bit Quantization&lt;/h4&gt;
&lt;p&gt;\(r=S(q-Z)\)&lt;/p&gt;

&lt;p&gt;\(S \rightarrow\) &lt;em&gt;“Scale”&lt;/em&gt; is arbitrary positive number. In software, it is a floating-point number just like the real value \(r\), of type &lt;code class=&quot;highlighter-rouge&quot;&gt;float&lt;/code&gt;. Note that for inference, the floating-point quantities need to be eliminated (discussed ahead).&lt;/p&gt;

&lt;p&gt;\(Z \rightarrow\) &lt;em&gt;“Zero-point”&lt;/em&gt; is the &lt;em&gt;quantized value&lt;/em&gt; corresponding to \(0\), and is of the same type as \(q\) i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;uint8&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Using the above mapping, 0 is exactly representable in real value as well. NN implementation often has 0-padding of arrays around boundaries. This property of the mapping facilitates in that.&lt;/p&gt;

&lt;h4 id=&quot;how-to-do-integer-arithmetic-only-matrix-multiplication&quot;&gt;How to do Integer-Arithmetic-Only Matrix Multiplication?&lt;/h4&gt;
&lt;p&gt;Currently, \(r\) and \(S\) are floating point, and we need to have an integer-arithmetic-only inference scheme using the mapping \(r=S(q-Z)\).&lt;/p&gt;

&lt;p&gt;Given, \(N \times N\) matrix composed of real values \(r_1\) and \(r_2\). Their product matrix has values \(r_3 = r_1r_2\). We have \(r_{\alpha}^{i,j}\) such that, \(1 \leq i,j \leq N\), with quantization parameters \((S_\alpha, Z_\alpha)\).&lt;/p&gt;

\[r_{\alpha}^{i,j} = S_\alpha(q_{\alpha}^{i,j} - Z_\alpha)\]

&lt;p&gt;By performing matrix multiplication, we have:&lt;/p&gt;

\[S_3(q_{3}^{i,k} - Z_3) = \sum_{j=1}^N S_1(q_{1}^{i,j} - Z_1)S_2(q_{2}^{j,k} - Z_2)\]

&lt;p&gt;rewriting,&lt;/p&gt;

\[q_{3}^{i,k} = Z_3 + M\sum_{j=1}^N (q_{1}^{i,j} - Z_1)(q_{2}^{j,k} - Z_2)\]

&lt;p&gt;where \(M\) remains the only non-integer constant which can be calculated offline using quantization scales \(S_1, S_2\) and \(S_3\),&lt;/p&gt;

\[M := \frac{S_1S_2}{S_3}\]

&lt;p&gt;Empirically, it is determined that 0 \lt M \lt 1, and therefore can be expressed in the normalized form as follows:&lt;/p&gt;

\[M = 2^{-n}M_0\]

&lt;p&gt;where, \(M_0 \in (0.5,1]\) and \(n \in \mathbb I^{+}\). The normalized \(M_0\) can be expressed as a fixed-point multiplier e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;int16&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;int32&lt;/code&gt; depending on hardware. E.g. if &lt;code class=&quot;highlighter-rouge&quot;&gt;int32&lt;/code&gt; is used the \(M_0\) will be the value closest to \(2^{31}M_0\). Also, since \(M_0 \gt 0.5\), so \(M_0\) has atleast 30bits of relative accuracy.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; This way, multiplication operation with \(M_0\) can be expressed as fixed point multiplication and the multiplication with \(2^{-n}\) can be done by bit-shifts with correct round-to-nearest behaviour.&lt;/p&gt;

&lt;h4 id=&quot;efficient-handing-of-zero-points&quot;&gt;Efficient Handing of Zero-points&lt;/h4&gt;

&lt;h4 id=&quot;implemention-of-typical-fused-layer&quot;&gt;Implemention of typical fused layer&lt;/h4&gt;

&lt;h3 id=&quot;training-with-simulated-quantization&quot;&gt;Training with Simulated Quantization&lt;/h3&gt;

&lt;h3 id=&quot;experiment-and-results&quot;&gt;Experiment and Results&lt;/h3&gt;

&lt;h2 id=&quot;analysis-strengths-and-weaknesses&quot;&gt;Analysis: Strengths and Weaknesses&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;This paper presents a detailed approach to reducing the inference latency on devices with limited memory and compute power. Unlike using an improper baseline architecture such as AlexNet, GoogleNet etc. to compare which does not reflect the advantage for mobile devices, this paper presents the improvement on MobileNet architectures, which makes the results significantly important for the Computer Architecture community.&lt;/li&gt;
  &lt;li&gt;Another important strength of the paper is that if presents the co-design framework and the experimental implementation on prominent mobile CPU for both training and inference using the proposed methodology i.e. the &lt;em&gt;quantization scheme&lt;/em&gt;. The benchmarking is done using ImageNet and COCO, which present a useful comparison of accuracy between the tradition FP-hardware and Integer-arithmetic hardware.&lt;/li&gt;
  &lt;li&gt;The weakness of the paper is that, it does not discuss about efficient integer-arithmetic representation for varying weight/activation vector sizes. Such as, why was 8-bit for weights and activation chosen, what would be the effect of chosing a higher bit counts, why does bias get 32-bit etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ideas-and-future-directions&quot;&gt;Ideas and Future Directions&lt;/h2&gt;

&lt;h2 id=&quot;lessons-learned-and-questions&quot;&gt;Lessons learned and Questions?&lt;/h2&gt;
&lt;p&gt;The only question I’m left pondering is wheter a mathematical model can be derived for comparing the FP vs Integer hardware? i.e. a theoretical comparison of accuracy of FP arithemetic approach against the expected deterioration of the proposed approach, and how much error cane be accomodated without compensating for the overall accuracy of the inference.&lt;/p&gt;

&lt;h2 id=&quot;some-other-interesting-approaches&quot;&gt;Some other interesting approaches&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H. and Kalenichenko, D., 2018. &lt;em&gt;“Quantization and training of neural networks for efficient integer-arithmetic-only inference”&lt;/em&gt;. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2704-2713).&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Gupta, Suyog, et al. “Deep learning with limited numerical precision.” International Conference on Machine Learning. 2015. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao. “Improving the speed of neural networks on CPUs.” (2011). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Deng, Jia, et al. “Imagenet: A large-scale hierarchical image database.” 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lin, Tsung-Yi, et al. “Microsoft coco: Common objects in context.” European conference on computer vision. Springer, Cham, 2014. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Explaination Required. Further explained in Appendix B. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Amitabh Yadav</name></author><category term="Machine Learning Hardware" /><category term="machine learning" /><category term="convolutional neural networks" /><category term="machine learning quantization" /><summary type="html">Summary Modern CNN architectures have very high model complexity and demard high computational efficiency. Mobile devices, however, present the challenge to accomodate within limited memory and meeting low latency to maintina user engagement. The cost of performing inference on mobile devices like, smartphones, AR/VR devices, drones etc. bears large computation and memory overhead. In order to have more efficient inferece hardware on mobile devices with integer-only arithmetic hardware, a quantization scheme helps in developing integer-only arithmetic to substitute the more sophesticated floating-point arithmetic hardware. Quantization, therefore, is the tradeoff between accuracy and on-device latency. The task at hand is to reduce model sizes and inference times, with minimal loss to accuracy. This also significantly affects the training procedure and requires the computer architect to maintain end-to-end model accuracy. This paper presents such a quantization scheme along with a hardware/software co-design training procedure. The improvements are demonstracted on MobileNet (a model family for run-time efficiency) running ImageNet classification and COCO object detection on modern CPUs. Contributions of the paper: A quantization scheme to quantize weights and activations as 8-bit integers, and other few parameters (such as, bias vectors) as 32-bit integers. This quantization scheme is derived from Ref.1 suggesting fixed-point arithemetic to accelerate training speed and Ref.2 suggesting 8-bit fixed-point arithemetic to accelerate inference on x86 architecture. A quantization inference framework for integer-arithmetic only hardware. A co-designed quantization training framework to maintain accuracy of inferece. Presents the implementation of the frameworks onMobileNetrunning on ARM CPUs, to perform classification (ImageNet3) and object detection (COCO 4). Details The quantization scheme employs an Integer-arithmetic only for Inference; and a Floating-point arithmetic for training. Both representation have a high degree of correlation with each other by separate adoption of the quantization scheme for each. \(q \rightarrow\) quantized value, denotes bit-representation of values. \(r \rightarrow\) real value, denotes the actual numerical value. The integer value to quantized value mapping is given as follows: \[r=S(q-Z)\] where, \(S\) and \(Z\) are some constants called quantization parameters. q can be quantized as B-bit integer for B-bit quantization. Here, B is 8-bits. Bias vectors are quantized as 32-bit integers. Here, a single set of quantization parameters is used for both weights array and activations array. Separate arrays can use separate quantization parameters. The mapping can be implemented as SIMD as opposed to the alternate method of using look-up table, to have better performance. 8-bit Quantization \(r=S(q-Z)\) \(S \rightarrow\) “Scale” is arbitrary positive number. In software, it is a floating-point number just like the real value \(r\), of type float. Note that for inference, the floating-point quantities need to be eliminated (discussed ahead). \(Z \rightarrow\) “Zero-point” is the quantized value corresponding to \(0\), and is of the same type as \(q\) i.e. uint8. Using the above mapping, 0 is exactly representable in real value as well. NN implementation often has 0-padding of arrays around boundaries. This property of the mapping facilitates in that. How to do Integer-Arithmetic-Only Matrix Multiplication? Currently, \(r\) and \(S\) are floating point, and we need to have an integer-arithmetic-only inference scheme using the mapping \(r=S(q-Z)\). Given, \(N \times N\) matrix composed of real values \(r_1\) and \(r_2\). Their product matrix has values \(r_3 = r_1r_2\). We have \(r_{\alpha}^{i,j}\) such that, \(1 \leq i,j \leq N\), with quantization parameters \((S_\alpha, Z_\alpha)\). \[r_{\alpha}^{i,j} = S_\alpha(q_{\alpha}^{i,j} - Z_\alpha)\] By performing matrix multiplication, we have: \[S_3(q_{3}^{i,k} - Z_3) = \sum_{j=1}^N S_1(q_{1}^{i,j} - Z_1)S_2(q_{2}^{j,k} - Z_2)\] rewriting, \[q_{3}^{i,k} = Z_3 + M\sum_{j=1}^N (q_{1}^{i,j} - Z_1)(q_{2}^{j,k} - Z_2)\] where \(M\) remains the only non-integer constant which can be calculated offline using quantization scales \(S_1, S_2\) and \(S_3\), \[M := \frac{S_1S_2}{S_3}\] Empirically, it is determined that 0 \lt M \lt 1, and therefore can be expressed in the normalized form as follows: \[M = 2^{-n}M_0\] where, \(M_0 \in (0.5,1]\) and \(n \in \mathbb I^{+}\). The normalized \(M_0\) can be expressed as a fixed-point multiplier e.g. int16 or int32 depending on hardware. E.g. if int32 is used the \(M_0\) will be the value closest to \(2^{31}M_0\). Also, since \(M_0 \gt 0.5\), so \(M_0\) has atleast 30bits of relative accuracy.5 This way, multiplication operation with \(M_0\) can be expressed as fixed point multiplication and the multiplication with \(2^{-n}\) can be done by bit-shifts with correct round-to-nearest behaviour. Efficient Handing of Zero-points Implemention of typical fused layer Training with Simulated Quantization Experiment and Results Analysis: Strengths and Weaknesses This paper presents a detailed approach to reducing the inference latency on devices with limited memory and compute power. Unlike using an improper baseline architecture such as AlexNet, GoogleNet etc. to compare which does not reflect the advantage for mobile devices, this paper presents the improvement on MobileNet architectures, which makes the results significantly important for the Computer Architecture community. Another important strength of the paper is that if presents the co-design framework and the experimental implementation on prominent mobile CPU for both training and inference using the proposed methodology i.e. the quantization scheme. The benchmarking is done using ImageNet and COCO, which present a useful comparison of accuracy between the tradition FP-hardware and Integer-arithmetic hardware. The weakness of the paper is that, it does not discuss about efficient integer-arithmetic representation for varying weight/activation vector sizes. Such as, why was 8-bit for weights and activation chosen, what would be the effect of chosing a higher bit counts, why does bias get 32-bit etc. Ideas and Future Directions Lessons learned and Questions? The only question I’m left pondering is wheter a mathematical model can be derived for comparing the FP vs Integer hardware? i.e. a theoretical comparison of accuracy of FP arithemetic approach against the expected deterioration of the proposed approach, and how much error cane be accomodated without compensating for the overall accuracy of the inference. Some other interesting approaches Gupta, Suyog, et al. “Deep learning with limited numerical precision.” International Conference on Machine Learning. 2015. &amp;#8617; Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao. “Improving the speed of neural networks on CPUs.” (2011). &amp;#8617; Deng, Jia, et al. “Imagenet: A large-scale hierarchical image database.” 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009. &amp;#8617; Lin, Tsung-Yi, et al. “Microsoft coco: Common objects in context.” European conference on computer vision. Springer, Cham, 2014. &amp;#8617; Explaination Required. Further explained in Appendix B. &amp;#8617;</summary></entry><entry><title type="html">Book Review: The Emperor’s New Mind by Roger Penrose</title><link href="http://localhost:4000/Book-Review-The-Emperors-New-Mind/" rel="alternate" type="text/html" title="Book Review: The Emperor’s New Mind by Roger Penrose" /><published>2020-04-12T00:00:00+02:00</published><updated>2020-04-12T00:00:00+02:00</updated><id>http://localhost:4000/Book-Review-The-Emperors-New-Mind</id><content type="html" xml:base="http://localhost:4000/Book-Review-The-Emperors-New-Mind/">&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Will be updated when the book is finished (Estimated: April 30th, 2020).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Please note that, this website also acts as my personal blog and an online notepad, where I prefer to take daily notes (using the Jekyll framework). Sp, the incomplete articles get updated in due course of time.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Chapter 1 presents a holistic view of the Turing Test. We dwell into the details to have the absolute test for an “indistinguishable from human” AI. The Turing Test is one such strong framework for testing an AI system where ‘clever methods’ used by the inquiring human can in practice present a hard situations for the AI at test. How relevant an AI’s behaviour is to act with ‘human flaws’ and how it behaves when posed with the ‘ridiculous’ questions are some of the hard tests within the Turing Test framework. We also talk about consiousness of AI and the concept of ‘pleasure’ and ‘pain’ for an AI. An AI can probably have a -100 to +100 scale for denoting pain and pleasure, respectively. Though what would happen in cases when human’s act impulsively without receiving the complete apprehension of pleasure or pain. The analogy used is that when we are about to touch as hot grill, the reation is impulsive without the need for quantizing pain. This chapter lays the foundation for the following chapters that’ll dwell deeper into conciousness, first from a computer science/mathematical point of view and later in the quantum mechanics framework.&lt;/p&gt;

&lt;p&gt;Chapter 2 goes into details of the Turing Machine as an implementation, and the need and significannce of having abstract concept of Turing Machine with the ‘infinite tape’ of I/O for performing computations. E.g. Euclid’s Algorithm for finding HCF is one of the oldest algorithms whose number of steps grows infinitely with the size of numbers. Euclid’s Algorithm was there even before Al Khwarizmi’s book on Algebra came from, that is the origin of the term, &lt;em&gt;Algorithm&lt;/em&gt;. Turing Machine is a abstract mathematical concept by Alan Turing in 1935-36 which originated from Hilbert’s posed problem called, &lt;em&gt;Entscheidungsproblem&lt;/em&gt; in 1928. The markings on the I/O tape of Turing machine can be considered in binary (for corresponding Denary (base-10) notation). Kurt Godel&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;currently reading.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;will be updated in due course of April 2020.&lt;/em&gt;&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="Book Review" /><category term="Roger Penrose" /><category term="The Emperor's New Mind" /><category term="Book Review" /><summary type="html">Review Will be updated when the book is finished (Estimated: April 30th, 2020). Please note that, this website also acts as my personal blog and an online notepad, where I prefer to take daily notes (using the Jekyll framework). Sp, the incomplete articles get updated in due course of time. Summary Chapter 1 presents a holistic view of the Turing Test. We dwell into the details to have the absolute test for an “indistinguishable from human” AI. The Turing Test is one such strong framework for testing an AI system where ‘clever methods’ used by the inquiring human can in practice present a hard situations for the AI at test. How relevant an AI’s behaviour is to act with ‘human flaws’ and how it behaves when posed with the ‘ridiculous’ questions are some of the hard tests within the Turing Test framework. We also talk about consiousness of AI and the concept of ‘pleasure’ and ‘pain’ for an AI. An AI can probably have a -100 to +100 scale for denoting pain and pleasure, respectively. Though what would happen in cases when human’s act impulsively without receiving the complete apprehension of pleasure or pain. The analogy used is that when we are about to touch as hot grill, the reation is impulsive without the need for quantizing pain. This chapter lays the foundation for the following chapters that’ll dwell deeper into conciousness, first from a computer science/mathematical point of view and later in the quantum mechanics framework. Chapter 2 goes into details of the Turing Machine as an implementation, and the need and significannce of having abstract concept of Turing Machine with the ‘infinite tape’ of I/O for performing computations. E.g. Euclid’s Algorithm for finding HCF is one of the oldest algorithms whose number of steps grows infinitely with the size of numbers. Euclid’s Algorithm was there even before Al Khwarizmi’s book on Algebra came from, that is the origin of the term, Algorithm. Turing Machine is a abstract mathematical concept by Alan Turing in 1935-36 which originated from Hilbert’s posed problem called, Entscheidungsproblem in 1928. The markings on the I/O tape of Turing machine can be considered in binary (for corresponding Denary (base-10) notation). Kurt Godel currently reading. will be updated in due course of April 2020.</summary></entry><entry><title type="html">Reiterating Quantum Algorithmic approach with the twist of structurality</title><link href="http://localhost:4000/Reiterating-Quantum-Algorithmic-approach/" rel="alternate" type="text/html" title="Reiterating Quantum Algorithmic approach with the twist of structurality" /><published>2020-04-08T00:00:00+02:00</published><updated>2020-04-08T00:00:00+02:00</updated><id>http://localhost:4000/Reiterating-Quantum-Algorithmic-approach</id><content type="html" xml:base="http://localhost:4000/Reiterating-Quantum-Algorithmic-approach/">&lt;p&gt;Back in February, Andras GILYEN’s talks, at Simon’s Institute in Berkeley, on Quantum Algorithms was enlightening for me, therefore I decided to transcribe a detailed account of methods and techniques Andras presented. Besides an overview, the talk discusses the more recently discovered applications, such as Singular Value Transformation. The characteristics of quantum algorithms that render ‘exponential speed-up’ such as the primitive Deutsch-Jozsa Algorithm (to determine balanced versus constant functions) along with numerous techniques developed later can be generalized to better understand and develop diverse applications, for example Convex Optimization.&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="Quantum Algorithms" /><category term="Quantum Computing" /><category term="Quantum Algorithms" /><summary type="html">Back in February, Andras GILYEN’s talks, at Simon’s Institute in Berkeley, on Quantum Algorithms was enlightening for me, therefore I decided to transcribe a detailed account of methods and techniques Andras presented. Besides an overview, the talk discusses the more recently discovered applications, such as Singular Value Transformation. The characteristics of quantum algorithms that render ‘exponential speed-up’ such as the primitive Deutsch-Jozsa Algorithm (to determine balanced versus constant functions) along with numerous techniques developed later can be generalized to better understand and develop diverse applications, for example Convex Optimization.</summary></entry><entry><title type="html">Processing in Memory: A workload driven perspective</title><link href="http://localhost:4000/processing-in-memory-a-workload-driven-perspective/" rel="alternate" type="text/html" title="Processing in Memory: A workload driven perspective" /><published>2020-04-07T00:00:00+02:00</published><updated>2020-04-07T00:00:00+02:00</updated><id>http://localhost:4000/processing-in-memory-a-workload-driven-perspective</id><content type="html" xml:base="http://localhost:4000/processing-in-memory-a-workload-driven-perspective/">&lt;p&gt;Performance gap between Memory (DRAM latencies) and Processor due to technology scaling is the motivation to investigate &lt;em&gt;Processing in Memory (PIM)&lt;/em&gt; architectures, also known as &lt;em&gt;Near Data Processing&lt;/em&gt;. Energy, Performance and Cost must be optimized. PIM tackles this by bringing computation to the data (and removing the data movement latencies). PIM is both: Completely new architectures and varying degree of architectural modifications in Memory Subsystems. This paper discusses the application of PIM in real-world applications (such as, Machine Learning, Data analytics, genome sequencing etc.), how to design the programming framework and the challenge of adoption of the new framework among the developer community.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Data moves from the Memory to the CPU via the &lt;em&gt;memory channel&lt;/em&gt; (a pin-limited off-chip Bus e.g. &lt;em&gt;double data-rate&lt;/em&gt; Memories aka &lt;em&gt;DDR&lt;/em&gt; use a 64-bit memory channel.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The CPU issues request to the &lt;em&gt;Memory Controller&lt;/em&gt;, which issues command across the memory channel to the DRAM.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The DRAM then reads the data and moves across the memory channel to the CPU (where the data has to travel through the memory hierarchy into the CPU’s register).&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Therefore, we need to rethink the computer architecture. PIM is one of such methods. The idea is almost 40 years old, but the technology was not mature enough to integrate a Memory with Processor elements. Technology such as 3D Stacked Memory (combining DRAM &lt;em&gt;Memory layers&lt;/em&gt; connected using through-layer &lt;em&gt;via&lt;/em&gt; along with a &lt;em&gt;logic layer&lt;/em&gt;), and more-computation friendly &lt;em&gt;resistive memory technologies&lt;/em&gt;, makes it possible to embed general-purpose computation directly within the memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Challenges:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Identification of application properties that can benefit from PIM architectures.&lt;/li&gt;
  &lt;li&gt;Making the architecture heterogeneous requires understanding of: a) architectural constraints (area, energy limitations along with the logic that is implementable within the memory), and b) application properties, such as memory access patterns and shared-data across different functions.&lt;/li&gt;
  &lt;li&gt;Therefore we need to understand the partition between PIM logic and CPU driven logic, establish the interfaces and mechanism for programming (while trying to stay close to the conventional programming model).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overview-of-pim&quot;&gt;Overview of PIM&lt;/h2&gt;

&lt;p&gt;Quantization Effectiveness:&lt;/p&gt;

&lt;h2 id=&quot;opportunities-in-pim-applications&quot;&gt;Opportunities in PIM applications&lt;/h2&gt;

&lt;h2 id=&quot;key-issues-in-programming-pim-architectures&quot;&gt;Key Issues in Programming PIM architectures&lt;/h2&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;

&lt;h2 id=&quot;future-challenges&quot;&gt;Future Challenges&lt;/h2&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;lessons-learnt&quot;&gt;Lessons Learnt&lt;/h2&gt;

&lt;h2 id=&quot;pros-and-cons-of-the-paper&quot;&gt;Pros and Cons of the Paper&lt;/h2&gt;

&lt;h2 id=&quot;improvement-ideas&quot;&gt;Improvement Ideas&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Memory Bottleneck:&lt;/strong&gt; Moving large amount of data for High-Performance and Data-Intense applications causes the bottleneck on energy and performance of the processor. The limited size of memory channel limits the number of access requests that can be issued in parallel, and the Random access patterns often leads to inefficient caching. The total cost of computation (in terms of performance and energy) is dominated by the cost of data movement. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Amitabh Yadav</name></author><category term="Computer Architecture" /><category term="Memory" /><category term="computer architecture" /><category term="in-memory computing" /><category term="3d ram" /><category term="memristor" /><category term="processing in memory" /><category term="pim architectures" /><summary type="html">Performance gap between Memory (DRAM latencies) and Processor due to technology scaling is the motivation to investigate Processing in Memory (PIM) architectures, also known as Near Data Processing. Energy, Performance and Cost must be optimized. PIM tackles this by bringing computation to the data (and removing the data movement latencies). PIM is both: Completely new architectures and varying degree of architectural modifications in Memory Subsystems. This paper discusses the application of PIM in real-world applications (such as, Machine Learning, Data analytics, genome sequencing etc.), how to design the programming framework and the challenge of adoption of the new framework among the developer community. Introduction Data moves from the Memory to the CPU via the memory channel (a pin-limited off-chip Bus e.g. double data-rate Memories aka DDR use a 64-bit memory channel.) The CPU issues request to the Memory Controller, which issues command across the memory channel to the DRAM. The DRAM then reads the data and moves across the memory channel to the CPU (where the data has to travel through the memory hierarchy into the CPU’s register).1 Therefore, we need to rethink the computer architecture. PIM is one of such methods. The idea is almost 40 years old, but the technology was not mature enough to integrate a Memory with Processor elements. Technology such as 3D Stacked Memory (combining DRAM Memory layers connected using through-layer via along with a logic layer), and more-computation friendly resistive memory technologies, makes it possible to embed general-purpose computation directly within the memory. Challenges: Identification of application properties that can benefit from PIM architectures. Making the architecture heterogeneous requires understanding of: a) architectural constraints (area, energy limitations along with the logic that is implementable within the memory), and b) application properties, such as memory access patterns and shared-data across different functions. Therefore we need to understand the partition between PIM logic and CPU driven logic, establish the interfaces and mechanism for programming (while trying to stay close to the conventional programming model). Overview of PIM Quantization Effectiveness: Opportunities in PIM applications Key Issues in Programming PIM architectures Related Work Future Challenges Conclusion Lessons Learnt Pros and Cons of the Paper Improvement Ideas Memory Bottleneck: Moving large amount of data for High-Performance and Data-Intense applications causes the bottleneck on energy and performance of the processor. The limited size of memory channel limits the number of access requests that can be issued in parallel, and the Random access patterns often leads to inefficient caching. The total cost of computation (in terms of performance and energy) is dominated by the cost of data movement. &amp;#8617;</summary></entry><entry><title type="html">ImageNet Classification with Deep Convolutional Neural Networks: AlexNet</title><link href="http://localhost:4000/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-AlexNet/" rel="alternate" type="text/html" title="ImageNet Classification with Deep Convolutional Neural Networks: AlexNet" /><published>2020-04-06T00:00:00+02:00</published><updated>2020-04-06T00:00:00+02:00</updated><id>http://localhost:4000/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-AlexNet</id><content type="html" xml:base="http://localhost:4000/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-AlexNet/">&lt;p&gt;&lt;img src=&quot;../images/cnn.jpeg&quot; alt=&quot;CNN for digit recognition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AlexNet, an application using Convolutional Neural Networks (CNN) for classifying images, famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% VS 26.2% (second place) error rates).&lt;/p&gt;

&lt;p&gt;The main contributions of the paper are as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Largest trained CNN model as of 2012. The dataset used for training was on the subsets of ImageNet.&lt;/li&gt;
  &lt;li&gt;A highly-optimized GPU implementation of 2D convolution and associated operations used in training CNN.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The AlexNet consists of 8 layers: 5 convolution and 3 fully-connected layers. The main features of AlexNet are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ReLU Non-Linearity: The Deep CNN using ReLU instead of the traditional \(f(x) = tanh(x)\) or \(f(x) = (1 +e^{−x})^{−1}\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Training on Multiple GPUs&lt;/li&gt;
  &lt;li&gt;Local Response Normalization&lt;/li&gt;
  &lt;li&gt;Overlapping Pooling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/alexnet-cnn.png&quot; alt=&quot;AlexNet CNN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;** Strengths of paper and mechanisms**
** Weaknesses of paper and mechanism**
** Detailed comments**
** Ideas for improvement**
** Lessons learned**&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. &lt;em&gt;“Imagenet classification with deep convolutional neural networks.”&lt;/em&gt; Advances in neural information processing systems. 2012.&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="Machine Learning" /><category term="machine learning" /><category term="convolutional neural networks" /><category term="alexnet" /><summary type="html">AlexNet, an application using Convolutional Neural Networks (CNN) for classifying images, famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% VS 26.2% (second place) error rates). The main contributions of the paper are as follows: Largest trained CNN model as of 2012. The dataset used for training was on the subsets of ImageNet. A highly-optimized GPU implementation of 2D convolution and associated operations used in training CNN. The AlexNet consists of 8 layers: 5 convolution and 3 fully-connected layers. The main features of AlexNet are: ReLU Non-Linearity: The Deep CNN using ReLU instead of the traditional \(f(x) = tanh(x)\) or \(f(x) = (1 +e^{−x})^{−1}\). Training on Multiple GPUs Local Response Normalization Overlapping Pooling ** Strengths of paper and mechanisms** ** Weaknesses of paper and mechanism** ** Detailed comments** ** Ideas for improvement** ** Lessons learned** Reference: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.</summary></entry><entry><title type="html">RAIDR: Retention-aware Intelligent DRAM Refresh</title><link href="http://localhost:4000/raidr-retention-aware-intelligent-dram-refresh/" rel="alternate" type="text/html" title="RAIDR: Retention-aware Intelligent DRAM Refresh" /><published>2020-04-05T00:00:00+02:00</published><updated>2020-04-05T00:00:00+02:00</updated><id>http://localhost:4000/raidr-retention-aware-intelligent-dram-refresh</id><content type="html" xml:base="http://localhost:4000/raidr-retention-aware-intelligent-dram-refresh/">&lt;p&gt;RAIDR is an intelligent method of adapting the DRAM memory refresh time to the refresh rate profile data of DRAM cells, which varies due to process variations. This allows the refresh rate not to remain constant i.e. determined by the weakest DRAM cell, but now it varies across the DRAM for different ‘Bins’. Based on the refresh rate of DRAM cells, the mechanism groups the DRAM cell rows into retention time bins, which is implemented using the Bloom filters implemented in the memory controller (low area overhead and no overflows). Bins with Weak DRAM cells, get refreshed more often than the one with good DRAM Cells.&lt;/p&gt;

&lt;h3 id=&quot;why-this-problem-is-important-to-deal-with&quot;&gt;Why this problem is important to deal with?&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;The refresh rate causes power consumption to increase.&lt;/li&gt;
  &lt;li&gt;Memory cannot be accessed during refresh i.e. denial of service.&lt;/li&gt;
  &lt;li&gt;Scaling the size of memory would increase the problem of refresh rate.&lt;/li&gt;
  &lt;li&gt;Only less than 1000 cells (out of 10e11) require refresh less than 256ms refresh, still the complete memory is refreshed every 64ms.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;strengths-of-paper-and-mechanisms&quot;&gt;Strengths of paper and mechanisms&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;With only 2 retention time bins (1.25 KB) implemented in the memory controller, there is a 74.6% refresh reduction, 16.1% average DRAM power reduction, and 8.6% average system performance improvement.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;weaknesses-of-paper-and-mechanism&quot;&gt;Weaknesses of paper and mechanism&lt;/h3&gt;

&lt;h3 id=&quot;detailed-comments&quot;&gt;Detailed comments&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;DRAM Cells store data in Capacitors. Over time, the capacitor loses charge. Therefore, data stored in DRAM is periodically read-out and rewritten, This is called DRAM Memory Refresh.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ideas-for-improvement&quot;&gt;Ideas for improvement&lt;/h3&gt;

&lt;h3 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Liu, Jamie, et al. &lt;em&gt;“RAIDR: Retention-aware intelligent DRAM refresh.”&lt;/em&gt; ACM SIGARCH Computer Architecture News 40.3 (2012): 1-12.&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="Memory" /><category term="computer architecture" /><category term="dram memory" /><category term="dram refresh" /><summary type="html">RAIDR is an intelligent method of adapting the DRAM memory refresh time to the refresh rate profile data of DRAM cells, which varies due to process variations. This allows the refresh rate not to remain constant i.e. determined by the weakest DRAM cell, but now it varies across the DRAM for different ‘Bins’. Based on the refresh rate of DRAM cells, the mechanism groups the DRAM cell rows into retention time bins, which is implemented using the Bloom filters implemented in the memory controller (low area overhead and no overflows). Bins with Weak DRAM cells, get refreshed more often than the one with good DRAM Cells. Why this problem is important to deal with? The refresh rate causes power consumption to increase. Memory cannot be accessed during refresh i.e. denial of service. Scaling the size of memory would increase the problem of refresh rate. Only less than 1000 cells (out of 10e11) require refresh less than 256ms refresh, still the complete memory is refreshed every 64ms. Strengths of paper and mechanisms With only 2 retention time bins (1.25 KB) implemented in the memory controller, there is a 74.6% refresh reduction, 16.1% average DRAM power reduction, and 8.6% average system performance improvement. Weaknesses of paper and mechanism Detailed comments DRAM Cells store data in Capacitors. Over time, the capacitor loses charge. Therefore, data stored in DRAM is periodically read-out and rewritten, This is called DRAM Memory Refresh. Ideas for improvement Lessons learned Reference: Liu, Jamie, et al. “RAIDR: Retention-aware intelligent DRAM refresh.” ACM SIGARCH Computer Architecture News 40.3 (2012): 1-12.</summary></entry><entry><title type="html">Machine Learning in High Energy Physics</title><link href="http://localhost:4000/machine-learning-in-high-energy-physics/" rel="alternate" type="text/html" title="Machine Learning in High Energy Physics" /><published>2020-04-04T00:00:00+02:00</published><updated>2020-04-04T00:00:00+02:00</updated><id>http://localhost:4000/machine-learning-in-high-energy-physics</id><content type="html" xml:base="http://localhost:4000/machine-learning-in-high-energy-physics/">&lt;p&gt;Event Size, Data Volume and Complexity of recorded data will present both, qualitative and quantitative, new challenges in the post-Higgs Boson era in Particle Physics. Computational resource utilization and algorithm efficiency would be the bottleneck that may limit the reach of Physics. Machine Learning is the currently the explored candidate to address both these issues.&lt;/p&gt;

&lt;p&gt;In this article, we discuss the Machine Learning (ML) applications in High Energy Physics (HEP), HEP-ML software, Hardware constraints for Training and Inference and, HEP-ML roadmap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;..\images\ml4hep_bkgrnd.png&quot; alt=&quot;Detector&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The targeted areas where ML can find applications for HEP, specifically Large Hadron Collider (LHC) research include:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Performance gains for Track Reconstruction and Analysis.&lt;/li&gt;
  &lt;li&gt;Reduce execution time for computationally expensive subroutines in event simulation, pattern recognition and calibration.&lt;/li&gt;
  &lt;li&gt;Real-time algorithms such as, Trigger (such as, L1 and HLT).&lt;/li&gt;
  &lt;li&gt;Reduction in data footprint with data compression, placement and access.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;what-does-the-lhc-do-exactly&quot;&gt;What does the LHC do exactly?&lt;/h3&gt;
&lt;p&gt;The challenge is to find rare events from the extremely high ‘pile up’ expected from the LHC. Probe the Standard Model, fundamental tests and search for new physics, by hunting for rare events in the background of extremely complex traces left behind due to proton bunch collisions.&lt;/p&gt;

&lt;h3 id=&quot;how-was-ml-used-prior-at-the-lhc&quot;&gt;How was ML used prior at the LHC?&lt;/h3&gt;
&lt;p&gt;Designed to work on large data-sets to reduce the complexity of data and find rare features/events. State-of-the-art implementations for event and particle identification, energy estimation and particle identification.&lt;/p&gt;

&lt;p&gt;The main ML algorithms currently used in particle physics are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Boosted Decision Trees (BDT)&lt;/li&gt;
  &lt;li&gt;Neural Networks (NN)
BDTs and NNs are mainly used for classification and regression in search for new particles by selecting relevant attributes/features from the data (which is signal and background events, usually has high pileup + noise).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Classification:&lt;/strong&gt; Supervised Learning, and prediction discrete valued output. For example, for classification of events and particles in data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regression:&lt;/strong&gt; Supervised Learning, by prediction continuous valued output. For example, to estimate energy of particle based on multiple measurements from multiple detectors.&lt;/p&gt;

&lt;p&gt;Training the model is the most expensive step, considering time to develop the model and time to train. Inference is usually inexpensive.&lt;/p&gt;

&lt;h3 id=&quot;what-is-the-role-for-deep-learning-at-lhc&quot;&gt;What is the role for Deep Learning at LHC?&lt;/h3&gt;
&lt;p&gt;Deep Learning (DL) is useful when we have for large data-sets, with large number of features, symmetries and complex non-linear input-output dependencies. The main DL architectures used in particle physics are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fully Connected Network (FCN)&lt;/li&gt;
  &lt;li&gt;Convolutional Neural Network (CNN)&lt;/li&gt;
  &lt;li&gt;Recurrent Neural Network (RNN)
Generative models are employed as well, to mimic multidimensional distributions to generate new instances, such as:&lt;/li&gt;
  &lt;li&gt;Variational Autoencoders (VAE)&lt;/li&gt;
  &lt;li&gt;Generative Adversarial Networks (GAN)
ML is also used for time series analysis and prediction, which is less relevant to HEP but is useful when quality of data and compute monitoring is essential, specially when time is an important aspect, such as, in event reconstruction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;targeted-areas-for-machine-learning-applications-and-research-for-hep&quot;&gt;Targeted areas for Machine Learning Applications and Research for HEP&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Detector Simulation:&lt;/strong&gt; New particles are discovered by comparing the recorded collision data with the predictions from Standard Model/beyond-standard Model physics. Detector Simulator, such as GEANT, help in simulation of particle trajectory and compare with the recorded data. The detector response along with known particle-matter iteration results, one can proceed to discover new particles. The HL-LHC would require simulation of up to trillions of events, that may help in testing the hypothesis. Simulation one proton-proton collision for LHC takes several minutes, which in addition to higher computational resource requirements, would scale many fold for HL-LHC simulations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Read-time analysis and Triggering:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Object Reconstruction, Identification and Calibration:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;End-to-end Deep Learning:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sustainable Matrix Element Method:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Matrix Element Machine Learning Method:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Learning the Standard Model:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Theory Applications:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Uncertainity Assignment:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Monitoring the Detectors, Hardware Anomaly and Preemptive Maintenance:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Computing Resource Optimization and Control of Networks and Productive Workflows:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;hep-machine-learning-software&quot;&gt;HEP Machine Learning Software&lt;/h3&gt;

&lt;h3 id=&quot;hardware-resources-and-computing-constraints&quot;&gt;Hardware Resources and Computing Constraints&lt;/h3&gt;

&lt;h3 id=&quot;hep-ml-roadmap-2017-2022&quot;&gt;HEP ML Roadmap (2017-2022)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Albertsson, Kim, et al. &lt;em&gt;“Machine learning in high energy physics community white paper.”&lt;/em&gt; Journal of Physics: Conference Series. Vol. 1085. No. 2. IOP Publishing, 2018.&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="High Energy Physics" /><category term="Machine Learning" /><category term="machine learning" /><category term="particle physics" /><category term="high energy physics" /><summary type="html">Event Size, Data Volume and Complexity of recorded data will present both, qualitative and quantitative, new challenges in the post-Higgs Boson era in Particle Physics. Computational resource utilization and algorithm efficiency would be the bottleneck that may limit the reach of Physics. Machine Learning is the currently the explored candidate to address both these issues. In this article, we discuss the Machine Learning (ML) applications in High Energy Physics (HEP), HEP-ML software, Hardware constraints for Training and Inference and, HEP-ML roadmap. The targeted areas where ML can find applications for HEP, specifically Large Hadron Collider (LHC) research include: Performance gains for Track Reconstruction and Analysis. Reduce execution time for computationally expensive subroutines in event simulation, pattern recognition and calibration. Real-time algorithms such as, Trigger (such as, L1 and HLT). Reduction in data footprint with data compression, placement and access. What does the LHC do exactly? The challenge is to find rare events from the extremely high ‘pile up’ expected from the LHC. Probe the Standard Model, fundamental tests and search for new physics, by hunting for rare events in the background of extremely complex traces left behind due to proton bunch collisions. How was ML used prior at the LHC? Designed to work on large data-sets to reduce the complexity of data and find rare features/events. State-of-the-art implementations for event and particle identification, energy estimation and particle identification. The main ML algorithms currently used in particle physics are: Boosted Decision Trees (BDT) Neural Networks (NN) BDTs and NNs are mainly used for classification and regression in search for new particles by selecting relevant attributes/features from the data (which is signal and background events, usually has high pileup + noise). Classification: Supervised Learning, and prediction discrete valued output. For example, for classification of events and particles in data. Regression: Supervised Learning, by prediction continuous valued output. For example, to estimate energy of particle based on multiple measurements from multiple detectors. Training the model is the most expensive step, considering time to develop the model and time to train. Inference is usually inexpensive. What is the role for Deep Learning at LHC? Deep Learning (DL) is useful when we have for large data-sets, with large number of features, symmetries and complex non-linear input-output dependencies. The main DL architectures used in particle physics are: Fully Connected Network (FCN) Convolutional Neural Network (CNN) Recurrent Neural Network (RNN) Generative models are employed as well, to mimic multidimensional distributions to generate new instances, such as: Variational Autoencoders (VAE) Generative Adversarial Networks (GAN) ML is also used for time series analysis and prediction, which is less relevant to HEP but is useful when quality of data and compute monitoring is essential, specially when time is an important aspect, such as, in event reconstruction. Targeted areas for Machine Learning Applications and Research for HEP Detector Simulation: New particles are discovered by comparing the recorded collision data with the predictions from Standard Model/beyond-standard Model physics. Detector Simulator, such as GEANT, help in simulation of particle trajectory and compare with the recorded data. The detector response along with known particle-matter iteration results, one can proceed to discover new particles. The HL-LHC would require simulation of up to trillions of events, that may help in testing the hypothesis. Simulation one proton-proton collision for LHC takes several minutes, which in addition to higher computational resource requirements, would scale many fold for HL-LHC simulations. Read-time analysis and Triggering: Object Reconstruction, Identification and Calibration: End-to-end Deep Learning: Sustainable Matrix Element Method: Matrix Element Machine Learning Method: Learning the Standard Model: Theory Applications: Uncertainity Assignment: Monitoring the Detectors, Hardware Anomaly and Preemptive Maintenance: Computing Resource Optimization and Control of Networks and Productive Workflows: HEP Machine Learning Software Hardware Resources and Computing Constraints HEP ML Roadmap (2017-2022)</summary></entry></feed>