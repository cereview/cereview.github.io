<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2020-04-14T07:26:45-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Amitabh’s Weblog</title><subtitle>C E R E V I E W</subtitle><author><name>Amitabh Yadav</name></author><entry><title type="html">Quantization and Training of NN for efficient Integer-Arithmetic-Only Inference</title><link href="http://localhost:4000/Quantization-and-Training-of-Neural-Networks-for-efficient-Integer-Arithmetic-Only-Inference/" rel="alternate" type="text/html" title="Quantization and Training of NN for efficient Integer-Arithmetic-Only Inference" /><published>2020-04-14T00:00:00-07:00</published><updated>2020-04-14T00:00:00-07:00</updated><id>http://localhost:4000/Quantization-and-Training-of-Neural-Networks-for-efficient-Integer-Arithmetic-Only-Inference</id><content type="html" xml:base="http://localhost:4000/Quantization-and-Training-of-Neural-Networks-for-efficient-Integer-Arithmetic-Only-Inference/">&lt;p&gt;Modern CNN architectures have very high model complexity and demard high computational efficiency. Mobile devices, however, present the challenge to accomodate within limited memory and meeting low latency to maintina user engagement. The cost of performing inference on mobile devices like, smartphones, AR/VR devices, drones etc. bears large computation and memory overhead. In order to have more efficient inferece hardware on mobile devices with integer-only arithmetic hardware, a &lt;em&gt;quantization scheme&lt;/em&gt; helps in developing integer-only arithmetic to substitute the more sophesticated floating-point arithmetic hardware. Quantization, therefore, is the tradeoff between accuracy and on-device latency. The task at hand is to reduce model sizes and inference times, with minimal loss to accuracy. This also significantly affects the training procedure and requires the computer architect to maintain end-to-end model accuracy.&lt;/p&gt;

&lt;p&gt;This paper presents such a &lt;em&gt;quantization scheme&lt;/em&gt; along with a hardware/software co-design training procedure. The improvements are demonstracted on MobileNet (a model family for run-time efficiency) running ImageNet classification and COCO object detection on modern CPUs.&lt;/p&gt;

&lt;h4 id=&quot;contributions-of-the-paper&quot;&gt;Contributions of the paper:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;A &lt;em&gt;quantization scheme&lt;/em&gt; to quantize &lt;em&gt;weights&lt;/em&gt; and &lt;em&gt;activations&lt;/em&gt; as 8-bit integers, and other few parameters (such as, bias vectors) as 32-bit integers. This quantization scheme is derived from Ref.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; suggesting fixed-point arithemetic to accelerate training speed and Ref.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; suggesting 8-bit fixed-point arithemetic to accelerate inference on x86 architecture.&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;quantization inference framework&lt;/em&gt; for integer-arithmetic only hardware.&lt;/li&gt;
  &lt;li&gt;A co-designed &lt;em&gt;quantization training framework&lt;/em&gt; to maintain accuracy of inferece.&lt;/li&gt;
  &lt;li&gt;Presents the implementation of the frameworks on&lt;em&gt;MobileNet&lt;/em&gt;running on ARM CPUs, to perform classification (ImageNet&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;) and object detection (COCO &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;
&lt;p&gt;The quantization scheme employs an Integer-arithmetic only for Inference; and a Floating-point arithmetic for training. Both representation have a high degree of correlation with each other by separate adoption of the &lt;em&gt;quantization scheme&lt;/em&gt; for each.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;q \rightarrow&lt;/script&gt; &lt;em&gt;quantized value&lt;/em&gt;, denotes bit-representation of values.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;r \rightarrow&lt;/script&gt; &lt;em&gt;real value&lt;/em&gt;, denotes the actual numerical value.&lt;/p&gt;

&lt;p&gt;The integer value to quantized value &lt;em&gt;mapping&lt;/em&gt; is given as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r=S(q-Z)&lt;/script&gt;

&lt;p&gt;where, &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; are some constants called &lt;em&gt;quantization parameters&lt;/em&gt;. q can be quantized as &lt;em&gt;B&lt;/em&gt;-bit integer for &lt;em&gt;B&lt;/em&gt;-bit quantization. Here, &lt;em&gt;B&lt;/em&gt; is 8-bits. Bias vectors are quantized as 32-bit integers.&lt;/p&gt;

&lt;p&gt;Here, a single set of quantization parameters is used for both &lt;em&gt;weights array&lt;/em&gt; and &lt;em&gt;activations array&lt;/em&gt;. Separate arrays can use separate quantization parameters. The mapping can be implemented as SIMD as opposed to the alternate method of using look-up table, to have better performance.&lt;/p&gt;

&lt;h4 id=&quot;8-bit-quantization&quot;&gt;8-bit Quantization&lt;/h4&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;r=S(q-Z)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;S \rightarrow&lt;/script&gt; &lt;em&gt;“Scale”&lt;/em&gt; is arbitrary positive number. In software, it is a floating-point number just like the real value &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;, of type &lt;code class=&quot;highlighter-rouge&quot;&gt;float&lt;/code&gt;. Note that for inference, the floating-point quantities need to be eliminated (discussed ahead).&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Z \rightarrow&lt;/script&gt; &lt;em&gt;“Zero-point”&lt;/em&gt; is the &lt;em&gt;quantized value&lt;/em&gt; corresponding to &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;, and is of the same type as &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;uint8&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Using the above mapping, 0 is exactly representable in real value as well. NN implementation often has 0-padding of arrays around boundaries. This property of the mapping facilitates in that.&lt;/p&gt;

&lt;h4 id=&quot;how-to-do-integer-arithmetic-only-matrix-multiplication&quot;&gt;How to do Integer-Arithmetic-Only Matrix Multiplication?&lt;/h4&gt;
&lt;p&gt;Currently, &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; are floating point, and we need to have an integer-arithmetic-only inference scheme using the mapping &lt;script type=&quot;math/tex&quot;&gt;r=S(q-Z)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Given, &lt;script type=&quot;math/tex&quot;&gt;N \times N&lt;/script&gt; matrix composed of real values &lt;script type=&quot;math/tex&quot;&gt;r_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;r_2&lt;/script&gt;. Their product matrix has values &lt;script type=&quot;math/tex&quot;&gt;r_3 = r_1r_2&lt;/script&gt;. We have &lt;script type=&quot;math/tex&quot;&gt;r_{\alpha}^{i,j}&lt;/script&gt; such that, &lt;script type=&quot;math/tex&quot;&gt;1 \leq i,j \leq N&lt;/script&gt;, with quantization parameters &lt;script type=&quot;math/tex&quot;&gt;(S_\alpha, Z_\alpha)&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r_{\alpha}^{i,j} = S_\alpha(q_{\alpha}^{i,j} - Z_\alpha)&lt;/script&gt;

&lt;p&gt;By performing matrix multiplication, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_3(q_{3}^{i,k} - Z_3) = \sum_{j=1}^N S_1(q_{1}^{i,j} - Z_1)S_2(q_{2}^{j,k} - Z_2)&lt;/script&gt;

&lt;p&gt;rewriting,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{3}^{i,k} = Z_3 + M\sum_{j=1}^N (q_{1}^{i,j} - Z_1)(q_{2}^{j,k} - Z_2)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; remains the only non-integer constant which can be calculated offline using quantization scales &lt;script type=&quot;math/tex&quot;&gt;S_1, S_2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;S_3&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M := \frac{S_1S_2}{S_3}&lt;/script&gt;

&lt;p&gt;Empirically, it is determined that 0 \lt M \lt 1, and therefore can be expressed in the normalized form as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M = 2^{-n}M_0&lt;/script&gt;

&lt;p&gt;where, &lt;script type=&quot;math/tex&quot;&gt;M_0 \in (0.5,1]&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;n \in \mathbb I^{+}&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;strengths-of-paper-and-mechanisms&quot;&gt;Strengths of paper and mechanisms&lt;/h4&gt;

&lt;h4 id=&quot;weaknesses-of-paper-and-mechanism&quot;&gt;Weaknesses of paper and mechanism&lt;/h4&gt;

&lt;h2 id=&quot;ideas-for-improvement&quot;&gt;Ideas for improvement&lt;/h2&gt;

&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h2&gt;

&lt;h2 id=&quot;some-other-interesting-approaches&quot;&gt;Some other interesting approaches&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H. and Kalenichenko, D., 2018. &lt;em&gt;“Quantization and training of neural networks for efficient integer-arithmetic-only inference”&lt;/em&gt;. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2704-2713).&lt;/p&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Gupta, Suyog, et al. “Deep learning with limited numerical precision.” International Conference on Machine Learning. 2015. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao. “Improving the speed of neural networks on CPUs.” (2011). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Deng, Jia, et al. “Imagenet: A large-scale hierarchical image database.” 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Lin, Tsung-Yi, et al. “Microsoft coco: Common objects in context.” European conference on computer vision. Springer, Cham, 2014. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Amitabh Yadav</name></author><category term="machine learning" /><category term="convolutional neural networks" /><category term="machine learning quantization" /><summary type="html">Modern CNN architectures have very high model complexity and demard high computational efficiency. Mobile devices, however, present the challenge to accomodate within limited memory and meeting low latency to maintina user engagement. The cost of performing inference on mobile devices like, smartphones, AR/VR devices, drones etc. bears large computation and memory overhead. In order to have more efficient inferece hardware on mobile devices with integer-only arithmetic hardware, a quantization scheme helps in developing integer-only arithmetic to substitute the more sophesticated floating-point arithmetic hardware. Quantization, therefore, is the tradeoff between accuracy and on-device latency. The task at hand is to reduce model sizes and inference times, with minimal loss to accuracy. This also significantly affects the training procedure and requires the computer architect to maintain end-to-end model accuracy. This paper presents such a quantization scheme along with a hardware/software co-design training procedure. The improvements are demonstracted on MobileNet (a model family for run-time efficiency) running ImageNet classification and COCO object detection on modern CPUs. Contributions of the paper: A quantization scheme to quantize weights and activations as 8-bit integers, and other few parameters (such as, bias vectors) as 32-bit integers. This quantization scheme is derived from Ref.1 suggesting fixed-point arithemetic to accelerate training speed and Ref.2 suggesting 8-bit fixed-point arithemetic to accelerate inference on x86 architecture. A quantization inference framework for integer-arithmetic only hardware. A co-designed quantization training framework to maintain accuracy of inferece. Presents the implementation of the frameworks onMobileNetrunning on ARM CPUs, to perform classification (ImageNet3) and object detection (COCO 4). Details The quantization scheme employs an Integer-arithmetic only for Inference; and a Floating-point arithmetic for training. Both representation have a high degree of correlation with each other by separate adoption of the quantization scheme for each. quantized value, denotes bit-representation of values. real value, denotes the actual numerical value. The integer value to quantized value mapping is given as follows: where, and are some constants called quantization parameters. q can be quantized as B-bit integer for B-bit quantization. Here, B is 8-bits. Bias vectors are quantized as 32-bit integers. Here, a single set of quantization parameters is used for both weights array and activations array. Separate arrays can use separate quantization parameters. The mapping can be implemented as SIMD as opposed to the alternate method of using look-up table, to have better performance. 8-bit Quantization “Scale” is arbitrary positive number. In software, it is a floating-point number just like the real value , of type float. Note that for inference, the floating-point quantities need to be eliminated (discussed ahead). “Zero-point” is the quantized value corresponding to , and is of the same type as i.e. uint8. Using the above mapping, 0 is exactly representable in real value as well. NN implementation often has 0-padding of arrays around boundaries. This property of the mapping facilitates in that. How to do Integer-Arithmetic-Only Matrix Multiplication? Currently, and are floating point, and we need to have an integer-arithmetic-only inference scheme using the mapping . Given, matrix composed of real values and . Their product matrix has values . We have such that, , with quantization parameters . By performing matrix multiplication, we have: rewriting, where remains the only non-integer constant which can be calculated offline using quantization scales and , Empirically, it is determined that 0 \lt M \lt 1, and therefore can be expressed in the normalized form as follows: where, and Gupta, Suyog, et al. “Deep learning with limited numerical precision.” International Conference on Machine Learning. 2015. &amp;#8617; Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao. “Improving the speed of neural networks on CPUs.” (2011). &amp;#8617; Deng, Jia, et al. “Imagenet: A large-scale hierarchical image database.” 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009. &amp;#8617; Lin, Tsung-Yi, et al. “Microsoft coco: Common objects in context.” European conference on computer vision. Springer, Cham, 2014. &amp;#8617;</summary></entry><entry><title type="html">Book Review: The Emperor’s New Mind by Roger Penrose</title><link href="http://localhost:4000/Book-Review-The-Emperors-New-Mind/" rel="alternate" type="text/html" title="Book Review: The Emperor's New Mind by Roger Penrose" /><published>2020-04-12T00:00:00-07:00</published><updated>2020-04-12T00:00:00-07:00</updated><id>http://localhost:4000/Book-Review-The-Emperors-New-Mind</id><content type="html" xml:base="http://localhost:4000/Book-Review-The-Emperors-New-Mind/">&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Will be updated when the book is finished (Estimated: April 30th, 2020).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Please note that, this website also acts as my personal blog and an online notepad, where I prefer to take daily notes (using the Jekyll framework). Sp, the incomplete articles get updated in due course of time.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Chapter 1 presents a holistic view of the Turing Test. We dwell into the details to have the absolute test for an “indistinguishable from human” AI. The Turing Test is one such strong framework for testing an AI system where ‘clever methods’ used by the inquiring human can in practice present a hard situations for the AI at test. How relevant an AI’s behaviour is to act with ‘human flaws’ and how it behaves when posed with the ‘ridiculous’ questions are some of the hard tests within the Turing Test framework. We also talk about consiousness of AI and the concept of ‘pleasure’ and ‘pain’ for an AI. An AI can probably have a -100 to +100 scale for denoting pain and pleasure, respectively. Though what would happen in cases when human’s act impulsively without receiving the complete apprehension of pleasure or pain. The analogy used is that when we are about to touch as hot grill, the reation is impulsive without the need for quantizing pain. This chapter lays the foundation for the following chapters that’ll dwell deeper into conciousness, first from a computer science/mathematical point of view and later in the quantum mechanics framework.&lt;/p&gt;

&lt;p&gt;Chapter 2 goes into details of the Turing Machine as an implementation, and the need and significannce of having abstract concept of Turing Machine with the ‘infinite tape’ of I/O for performing computations. E.g. Euclid’s Algorithm for finding HCF is one of the oldest algorithms whose number of steps grows infinitely with the size of numbers. Euclid’s Algorithm was there even before Al Khwarizmi’s book on Algebra came from, that is the origin of the term, &lt;em&gt;Algorithm&lt;/em&gt;. Turing Machine is a abstract mathematical concept by Alan Turing in 1935-36 which originated from Hilbert’s posed problem called, &lt;em&gt;Entscheidungsproblem&lt;/em&gt; in 1928. The markings on the I/O tape of Turing machine can be considered in binary (for corresponding Denary (base-10) notation). Kurt Godel&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;currently reading.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;will be updated in due course of April 2020.&lt;/em&gt;&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="Roger Penrose" /><category term="The Emperor's New Mind" /><category term="Book Review" /><summary type="html">Review Will be updated when the book is finished (Estimated: April 30th, 2020). Please note that, this website also acts as my personal blog and an online notepad, where I prefer to take daily notes (using the Jekyll framework). Sp, the incomplete articles get updated in due course of time. Summary Chapter 1 presents a holistic view of the Turing Test. We dwell into the details to have the absolute test for an “indistinguishable from human” AI. The Turing Test is one such strong framework for testing an AI system where ‘clever methods’ used by the inquiring human can in practice present a hard situations for the AI at test. How relevant an AI’s behaviour is to act with ‘human flaws’ and how it behaves when posed with the ‘ridiculous’ questions are some of the hard tests within the Turing Test framework. We also talk about consiousness of AI and the concept of ‘pleasure’ and ‘pain’ for an AI. An AI can probably have a -100 to +100 scale for denoting pain and pleasure, respectively. Though what would happen in cases when human’s act impulsively without receiving the complete apprehension of pleasure or pain. The analogy used is that when we are about to touch as hot grill, the reation is impulsive without the need for quantizing pain. This chapter lays the foundation for the following chapters that’ll dwell deeper into conciousness, first from a computer science/mathematical point of view and later in the quantum mechanics framework. Chapter 2 goes into details of the Turing Machine as an implementation, and the need and significannce of having abstract concept of Turing Machine with the ‘infinite tape’ of I/O for performing computations. E.g. Euclid’s Algorithm for finding HCF is one of the oldest algorithms whose number of steps grows infinitely with the size of numbers. Euclid’s Algorithm was there even before Al Khwarizmi’s book on Algebra came from, that is the origin of the term, Algorithm. Turing Machine is a abstract mathematical concept by Alan Turing in 1935-36 which originated from Hilbert’s posed problem called, Entscheidungsproblem in 1928. The markings on the I/O tape of Turing machine can be considered in binary (for corresponding Denary (base-10) notation). Kurt Godel currently reading. will be updated in due course of April 2020.</summary></entry><entry><title type="html">Reiterating Quantum Algorithmic approach with the twist of structurality</title><link href="http://localhost:4000/Reiterating-Quantum-Algorithmic-approach/" rel="alternate" type="text/html" title="Reiterating Quantum Algorithmic approach with the twist of structurality" /><published>2020-04-08T00:00:00-07:00</published><updated>2020-04-08T00:00:00-07:00</updated><id>http://localhost:4000/Reiterating-Quantum-Algorithmic-approach</id><content type="html" xml:base="http://localhost:4000/Reiterating-Quantum-Algorithmic-approach/">&lt;p&gt;Back in February, Andras GILYEN’s talks, at Simon’s Institute in Berkeley, on Quantum Algorithms was enlightening for me, therefore I decided to transcribe a detailed account of methods and techniques Andras presented. Besides an overview, the talk discusses the more recently discovered applications, such as Singular Value Transformation. The characteristics of quantum algorithms that render ‘exponential speed-up’ such as the primitive Deutsch-Jozsa Algorithm (to determine balanced versus constant functions) along with numerous techniques developed later can be generalized to better understand and develop diverse applications, for example Convex Optimization.&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="Quantum Computing" /><category term="Quantum Algorithms" /><summary type="html">Back in February, Andras GILYEN’s talks, at Simon’s Institute in Berkeley, on Quantum Algorithms was enlightening for me, therefore I decided to transcribe a detailed account of methods and techniques Andras presented. Besides an overview, the talk discusses the more recently discovered applications, such as Singular Value Transformation. The characteristics of quantum algorithms that render ‘exponential speed-up’ such as the primitive Deutsch-Jozsa Algorithm (to determine balanced versus constant functions) along with numerous techniques developed later can be generalized to better understand and develop diverse applications, for example Convex Optimization.</summary></entry><entry><title type="html">Processing in Memory: A workload driven perspective</title><link href="http://localhost:4000/processing-in-memory-a-workload-driven-perspective/" rel="alternate" type="text/html" title="Processing in Memory: A workload driven perspective" /><published>2020-04-07T00:00:00-07:00</published><updated>2020-04-07T00:00:00-07:00</updated><id>http://localhost:4000/processing-in-memory-a-workload-driven-perspective</id><content type="html" xml:base="http://localhost:4000/processing-in-memory-a-workload-driven-perspective/">&lt;p&gt;Performance gap between Memory (DRAM latencies) and Processor due to technology scaling is the motivation to investigate &lt;em&gt;Processing in Memory (PIM)&lt;/em&gt; architectures, also known as &lt;em&gt;Near Data Processing&lt;/em&gt;. Energy, Performance and Cost must be optimized. PIM tackles this by bringing computation to the data (and removing the data movement latencies). PIM is both: Completely new architectures and varying degree of architectural modifications in Memory Subsystems. This paper discusses the application of PIM in real-world applications (such as, Machine Learning, Data analytics, genome sequencing etc.), how to design the programming framework and the challenge of adoption of the new framework among the developer community.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Data moves from the Memory to the CPU via the &lt;em&gt;memory channel&lt;/em&gt; (a pin-limited off-chip Bus e.g. &lt;em&gt;double data-rate&lt;/em&gt; Memories aka &lt;em&gt;DDR&lt;/em&gt; use a 64-bit memory channel.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The CPU issues request to the &lt;em&gt;Memory Controller&lt;/em&gt;, which issues command across the memory channel to the DRAM.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The DRAM then reads the data and moves across the memory channel to the CPU (where the data has to travel through the memory hierarchy into the CPU’s register).&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Therefore, we need to rethink the computer architecture. PIM is one of such methods. The idea is almost 40 years old, but the technology was not mature enough to integrate a Memory with Processor elements. Technology such as 3D Stacked Memory (combining DRAM &lt;em&gt;Memory layers&lt;/em&gt; connected using through-layer &lt;em&gt;via&lt;/em&gt; along with a &lt;em&gt;logic layer&lt;/em&gt;), and more-computation friendly &lt;em&gt;resistive memory technologies&lt;/em&gt;, makes it possible to embed general-purpose computation directly within the memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Challenges:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Identification of application properties that can benefit from PIM architectures.&lt;/li&gt;
  &lt;li&gt;Making the architecture heterogeneous requires understanding of: a) architectural constraints (area, energy limitations along with the logic that is implementable within the memory), and b) application properties, such as memory access patterns and shared-data across different functions.&lt;/li&gt;
  &lt;li&gt;Therefore we need to understand the partition between PIM logic and CPU driven logic, establish the interfaces and mechanism for programming (while trying to stay close to the conventional programming model).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overview-of-pim&quot;&gt;Overview of PIM&lt;/h2&gt;

&lt;h2 id=&quot;opportunities-in-pim-applications&quot;&gt;Opportunities in PIM applications&lt;/h2&gt;

&lt;h2 id=&quot;key-issues-in-programming-pim-architectures&quot;&gt;Key Issues in Programming PIM architectures&lt;/h2&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;

&lt;h2 id=&quot;future-challenges&quot;&gt;Future Challenges&lt;/h2&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;lessons-learnt&quot;&gt;Lessons Learnt&lt;/h2&gt;

&lt;h2 id=&quot;pros-and-cons-of-the-paper&quot;&gt;Pros and Cons of the Paper&lt;/h2&gt;

&lt;h2 id=&quot;improvement-ideas&quot;&gt;Improvement Ideas&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Memory Bottleneck:&lt;/strong&gt; Moving large amount of data for High-Performance and Data-Intense applications causes the bottleneck on energy and performance of the processor. The limited size of memory channel limits the number of access requests that can be issued in parallel, and the Random access patterns often leads to inefficient caching. The total cost of computation (in terms of performance and energy) is dominated by the cost of data movement. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Amitabh Yadav</name></author><category term="computer architecture" /><category term="in-memory computing" /><category term="3d ram" /><category term="memristor" /><category term="processing in memory" /><category term="pim architectures" /><summary type="html">Performance gap between Memory (DRAM latencies) and Processor due to technology scaling is the motivation to investigate Processing in Memory (PIM) architectures, also known as Near Data Processing. Energy, Performance and Cost must be optimized. PIM tackles this by bringing computation to the data (and removing the data movement latencies). PIM is both: Completely new architectures and varying degree of architectural modifications in Memory Subsystems. This paper discusses the application of PIM in real-world applications (such as, Machine Learning, Data analytics, genome sequencing etc.), how to design the programming framework and the challenge of adoption of the new framework among the developer community. Introduction Data moves from the Memory to the CPU via the memory channel (a pin-limited off-chip Bus e.g. double data-rate Memories aka DDR use a 64-bit memory channel.) The CPU issues request to the Memory Controller, which issues command across the memory channel to the DRAM. The DRAM then reads the data and moves across the memory channel to the CPU (where the data has to travel through the memory hierarchy into the CPU’s register).1 Therefore, we need to rethink the computer architecture. PIM is one of such methods. The idea is almost 40 years old, but the technology was not mature enough to integrate a Memory with Processor elements. Technology such as 3D Stacked Memory (combining DRAM Memory layers connected using through-layer via along with a logic layer), and more-computation friendly resistive memory technologies, makes it possible to embed general-purpose computation directly within the memory. Challenges: Identification of application properties that can benefit from PIM architectures. Making the architecture heterogeneous requires understanding of: a) architectural constraints (area, energy limitations along with the logic that is implementable within the memory), and b) application properties, such as memory access patterns and shared-data across different functions. Therefore we need to understand the partition between PIM logic and CPU driven logic, establish the interfaces and mechanism for programming (while trying to stay close to the conventional programming model). Overview of PIM Opportunities in PIM applications Key Issues in Programming PIM architectures Related Work Future Challenges Conclusion Lessons Learnt Pros and Cons of the Paper Improvement Ideas Memory Bottleneck: Moving large amount of data for High-Performance and Data-Intense applications causes the bottleneck on energy and performance of the processor. The limited size of memory channel limits the number of access requests that can be issued in parallel, and the Random access patterns often leads to inefficient caching. The total cost of computation (in terms of performance and energy) is dominated by the cost of data movement. &amp;#8617;</summary></entry><entry><title type="html">ImageNet Classification with Deep Convolutional Neural Networks: AlexNet</title><link href="http://localhost:4000/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-AlexNet/" rel="alternate" type="text/html" title="ImageNet Classification with Deep Convolutional Neural Networks: AlexNet" /><published>2020-04-06T00:00:00-07:00</published><updated>2020-04-06T00:00:00-07:00</updated><id>http://localhost:4000/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-AlexNet</id><content type="html" xml:base="http://localhost:4000/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-AlexNet/">&lt;p&gt;&lt;img src=&quot;../images/cnn.jpeg&quot; alt=&quot;CNN for digit recognition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AlexNet, an application using Convolutional Neural Networks (CNN) for classifying images, famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% VS 26.2% (second place) error rates).&lt;/p&gt;

&lt;p&gt;The main contributions of the paper are as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Largest trained CNN model as of 2012. The dataset used for training was on the subsets of ImageNet.&lt;/li&gt;
  &lt;li&gt;A highly-optimized GPU implementation of 2D convolution and associated operations used in training CNN.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The AlexNet consists of 8 layers: 5 convolution and 3 fully-connected layers. The main features of AlexNet are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ReLU Non-Linearity: The Deep CNN using ReLU instead of the traditional &lt;script type=&quot;math/tex&quot;&gt;f(x) = tanh(x)&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;f(x) = (1 +e^{−x})^{−1}&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Training on Multiple GPUs&lt;/li&gt;
  &lt;li&gt;Local Response Normalization&lt;/li&gt;
  &lt;li&gt;Overlapping Pooling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/alexnet-cnn.png&quot; alt=&quot;AlexNet CNN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;** Strengths of paper and mechanisms**
** Weaknesses of paper and mechanism**
** Detailed comments**
** Ideas for improvement**
** Lessons learned**&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. &lt;em&gt;“Imagenet classification with deep convolutional neural networks.”&lt;/em&gt; Advances in neural information processing systems. 2012.&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="machine learning" /><category term="convolutional neural networks" /><category term="alexnet" /><summary type="html">AlexNet, an application using Convolutional Neural Networks (CNN) for classifying images, famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% VS 26.2% (second place) error rates). The main contributions of the paper are as follows: Largest trained CNN model as of 2012. The dataset used for training was on the subsets of ImageNet. A highly-optimized GPU implementation of 2D convolution and associated operations used in training CNN. The AlexNet consists of 8 layers: 5 convolution and 3 fully-connected layers. The main features of AlexNet are: ReLU Non-Linearity: The Deep CNN using ReLU instead of the traditional or . Training on Multiple GPUs Local Response Normalization Overlapping Pooling ** Strengths of paper and mechanisms** ** Weaknesses of paper and mechanism** ** Detailed comments** ** Ideas for improvement** ** Lessons learned** Reference: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.</summary></entry><entry><title type="html">RAIDR: Retention-aware Intelligent DRAM Refresh</title><link href="http://localhost:4000/raidr-retention-aware-intelligent-dram-refresh/" rel="alternate" type="text/html" title="RAIDR: Retention-aware Intelligent DRAM Refresh" /><published>2020-04-05T00:00:00-07:00</published><updated>2020-04-05T00:00:00-07:00</updated><id>http://localhost:4000/raidr-retention-aware-intelligent-dram-refresh</id><content type="html" xml:base="http://localhost:4000/raidr-retention-aware-intelligent-dram-refresh/">&lt;p&gt;RAIDR is an intelligent method of adapting the DRAM memory refresh time to the refresh rate profile data of DRAM cells, which varies due to process variations. This allows the refresh rate not to remain constant i.e. determined by the weakest DRAM cell, but now it varies across the DRAM for different ‘Bins’. Based on the refresh rate of DRAM cells, the mechanism groups the DRAM cell rows into retention time bins, which is implemented using the Bloom filters implemented in the memory controller (low area overhead and no overflows). Bins with Weak DRAM cells, get refreshed more often than the one with good DRAM Cells.&lt;/p&gt;

&lt;h3 id=&quot;why-this-problem-is-important-to-deal-with&quot;&gt;Why this problem is important to deal with?&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;The refresh rate causes power consumption to increase.&lt;/li&gt;
  &lt;li&gt;Memory cannot be accessed during refresh i.e. denial of service.&lt;/li&gt;
  &lt;li&gt;Scaling the size of memory would increase the problem of refresh rate.&lt;/li&gt;
  &lt;li&gt;Only less than 1000 cells (out of 10e11) require refresh less than 256ms refresh, still the complete memory is refreshed every 64ms.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;strengths-of-paper-and-mechanisms&quot;&gt;Strengths of paper and mechanisms&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;With only 2 retention time bins (1.25 KB) implemented in the memory controller, there is a 74.6% refresh reduction, 16.1% average DRAM power reduction, and 8.6% average system performance improvement.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;weaknesses-of-paper-and-mechanism&quot;&gt;Weaknesses of paper and mechanism&lt;/h3&gt;

&lt;h3 id=&quot;detailed-comments&quot;&gt;Detailed comments&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;DRAM Cells store data in Capacitors. Over time, the capacitor loses charge. Therefore, data stored in DRAM is periodically read-out and rewritten, This is called DRAM Memory Refresh.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ideas-for-improvement&quot;&gt;Ideas for improvement&lt;/h3&gt;

&lt;h3 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Liu, Jamie, et al. &lt;em&gt;“RAIDR: Retention-aware intelligent DRAM refresh.”&lt;/em&gt; ACM SIGARCH Computer Architecture News 40.3 (2012): 1-12.&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="computer architecture" /><category term="dram memory" /><category term="dram refresh" /><summary type="html">RAIDR is an intelligent method of adapting the DRAM memory refresh time to the refresh rate profile data of DRAM cells, which varies due to process variations. This allows the refresh rate not to remain constant i.e. determined by the weakest DRAM cell, but now it varies across the DRAM for different ‘Bins’. Based on the refresh rate of DRAM cells, the mechanism groups the DRAM cell rows into retention time bins, which is implemented using the Bloom filters implemented in the memory controller (low area overhead and no overflows). Bins with Weak DRAM cells, get refreshed more often than the one with good DRAM Cells. Why this problem is important to deal with? The refresh rate causes power consumption to increase. Memory cannot be accessed during refresh i.e. denial of service. Scaling the size of memory would increase the problem of refresh rate. Only less than 1000 cells (out of 10e11) require refresh less than 256ms refresh, still the complete memory is refreshed every 64ms. Strengths of paper and mechanisms With only 2 retention time bins (1.25 KB) implemented in the memory controller, there is a 74.6% refresh reduction, 16.1% average DRAM power reduction, and 8.6% average system performance improvement. Weaknesses of paper and mechanism Detailed comments DRAM Cells store data in Capacitors. Over time, the capacitor loses charge. Therefore, data stored in DRAM is periodically read-out and rewritten, This is called DRAM Memory Refresh. Ideas for improvement Lessons learned Reference: Liu, Jamie, et al. “RAIDR: Retention-aware intelligent DRAM refresh.” ACM SIGARCH Computer Architecture News 40.3 (2012): 1-12.</summary></entry><entry><title type="html">Machine Learning in High Energy Physics</title><link href="http://localhost:4000/machine-learning-in-high-energy-physics/" rel="alternate" type="text/html" title="Machine Learning in High Energy Physics" /><published>2020-04-04T00:00:00-07:00</published><updated>2020-04-04T00:00:00-07:00</updated><id>http://localhost:4000/machine-learning-in-high-energy-physics</id><content type="html" xml:base="http://localhost:4000/machine-learning-in-high-energy-physics/">&lt;p&gt;Event Size, Data Volume and Complexity of recorded data will present both, qualitative and quantitative, new challenges in the post-Higgs Boson era in Particle Physics. Computational resource utilization and algorithm efficiency would be the bottleneck that may limit the reach of Physics. Machine Learning is the currently the explored candidate to address both these issues.&lt;/p&gt;

&lt;p&gt;In this article, we discuss the Machine Learning (ML) applications in High Energy Physics (HEP), HEP-ML software, Hardware constraints for Training and Inference and, HEP-ML roadmap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;..\images\ml4hep_bkgrnd.png&quot; alt=&quot;Detector&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The targeted areas where ML can find applications for HEP, specifically Large Hadron Collider (LHC) research include:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Performance gains for Track Reconstruction and Analysis.&lt;/li&gt;
  &lt;li&gt;Reduce execution time for computationally expensive subroutines in event simulation, pattern recognition and calibration.&lt;/li&gt;
  &lt;li&gt;Real-time algorithms such as, Trigger (such as, L1 and HLT).&lt;/li&gt;
  &lt;li&gt;Reduction in data footprint with data compression, placement and access.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;what-does-the-lhc-do-exactly&quot;&gt;What does the LHC do exactly?&lt;/h3&gt;
&lt;p&gt;The challenge is to find rare events from the extremely high ‘pile up’ expected from the LHC. Probe the Standard Model, fundamental tests and search for new physics, by hunting for rare events in the background of extremely complex traces left behind due to proton bunch collisions.&lt;/p&gt;

&lt;h3 id=&quot;how-was-ml-used-prior-at-the-lhc&quot;&gt;How was ML used prior at the LHC?&lt;/h3&gt;
&lt;p&gt;Designed to work on large data-sets to reduce the complexity of data and find rare features/events. State-of-the-art implementations for event and particle identification, energy estimation and particle identification.&lt;/p&gt;

&lt;p&gt;The main ML algorithms currently used in particle physics are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Boosted Decision Trees (BDT)&lt;/li&gt;
  &lt;li&gt;Neural Networks (NN)
BDTs and NNs are mainly used for classification and regression in search for new particles by selecting relevant attributes/features from the data (which is signal and background events, usually has high pileup + noise).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Classification:&lt;/strong&gt; Supervised Learning, and prediction discrete valued output. For example, for classification of events and particles in data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regression:&lt;/strong&gt; Supervised Learning, by prediction continuous valued output. For example, to estimate energy of particle based on multiple measurements from multiple detectors.&lt;/p&gt;

&lt;p&gt;Training the model is the most expensive step, considering time to develop the model and time to train. Inference is usually inexpensive.&lt;/p&gt;

&lt;h3 id=&quot;what-is-the-role-for-deep-learning-at-lhc&quot;&gt;What is the role for Deep Learning at LHC?&lt;/h3&gt;
&lt;p&gt;Deep Learning (DL) is useful when we have for large data-sets, with large number of features, symmetries and complex non-linear input-output dependencies. The main DL architectures used in particle physics are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fully Connected Network (FCN)&lt;/li&gt;
  &lt;li&gt;Convolutional Neural Network (CNN)&lt;/li&gt;
  &lt;li&gt;Recurrent Neural Network (RNN)
Generative models are employed as well, to mimic multidimensional distributions to generate new instances, such as:&lt;/li&gt;
  &lt;li&gt;Variational Autoencoders (VAE)&lt;/li&gt;
  &lt;li&gt;Generative Adversarial Networks (GAN)
ML is also used for time series analysis and prediction, which is less relevant to HEP but is useful when quality of data and compute monitoring is essential, specially when time is an important aspect, such as, in event reconstruction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;targeted-areas-for-machine-learning-applications-and-research-for-hep&quot;&gt;Targeted areas for Machine Learning Applications and Research for HEP&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Detector Simulation:&lt;/strong&gt; New particles are discovered by comparing the recorded collision data with the predictions from Standard Model/beyond-standard Model physics. Detector Simulator, such as GEANT, help in simulation of particle trajectory and compare with the recorded data. The detector response along with known particle-matter iteration results, one can proceed to discover new particles. The HL-LHC would require simulation of up to trillions of events, that may help in testing the hypothesis. Simulation one proton-proton collision for LHC takes several minutes, which in addition to higher computational resource requirements, would scale many fold for HL-LHC simulations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Read-time analysis and Triggering:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Object Reconstruction, Identification and Calibration:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;End-to-end Deep Learning:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sustainable Matrix Element Method:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Matrix Element Machine Learning Method:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Learning the Standard Model:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Theory Applications:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Uncertainity Assignment:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Monitoring the Detectors, Hardware Anomaly and Preemptive Maintenance:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Computing Resource Optimization and Control of Networks and Productive Workflows:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;hep-machine-learning-software&quot;&gt;HEP Machine Learning Software&lt;/h3&gt;

&lt;h3 id=&quot;hardware-resources-and-computing-constraints&quot;&gt;Hardware Resources and Computing Constraints&lt;/h3&gt;

&lt;h3 id=&quot;hep-ml-roadmap-2017-2022&quot;&gt;HEP ML Roadmap (2017-2022)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; Albertsson, Kim, et al. &lt;em&gt;“Machine learning in high energy physics community white paper.”&lt;/em&gt; Journal of Physics: Conference Series. Vol. 1085. No. 2. IOP Publishing, 2018.&lt;/p&gt;</content><author><name>Amitabh Yadav</name></author><category term="machine learning" /><category term="particle physics" /><category term="high energy physics" /><summary type="html">Event Size, Data Volume and Complexity of recorded data will present both, qualitative and quantitative, new challenges in the post-Higgs Boson era in Particle Physics. Computational resource utilization and algorithm efficiency would be the bottleneck that may limit the reach of Physics. Machine Learning is the currently the explored candidate to address both these issues. In this article, we discuss the Machine Learning (ML) applications in High Energy Physics (HEP), HEP-ML software, Hardware constraints for Training and Inference and, HEP-ML roadmap. The targeted areas where ML can find applications for HEP, specifically Large Hadron Collider (LHC) research include: Performance gains for Track Reconstruction and Analysis. Reduce execution time for computationally expensive subroutines in event simulation, pattern recognition and calibration. Real-time algorithms such as, Trigger (such as, L1 and HLT). Reduction in data footprint with data compression, placement and access. What does the LHC do exactly? The challenge is to find rare events from the extremely high ‘pile up’ expected from the LHC. Probe the Standard Model, fundamental tests and search for new physics, by hunting for rare events in the background of extremely complex traces left behind due to proton bunch collisions. How was ML used prior at the LHC? Designed to work on large data-sets to reduce the complexity of data and find rare features/events. State-of-the-art implementations for event and particle identification, energy estimation and particle identification. The main ML algorithms currently used in particle physics are: Boosted Decision Trees (BDT) Neural Networks (NN) BDTs and NNs are mainly used for classification and regression in search for new particles by selecting relevant attributes/features from the data (which is signal and background events, usually has high pileup + noise). Classification: Supervised Learning, and prediction discrete valued output. For example, for classification of events and particles in data. Regression: Supervised Learning, by prediction continuous valued output. For example, to estimate energy of particle based on multiple measurements from multiple detectors. Training the model is the most expensive step, considering time to develop the model and time to train. Inference is usually inexpensive. What is the role for Deep Learning at LHC? Deep Learning (DL) is useful when we have for large data-sets, with large number of features, symmetries and complex non-linear input-output dependencies. The main DL architectures used in particle physics are: Fully Connected Network (FCN) Convolutional Neural Network (CNN) Recurrent Neural Network (RNN) Generative models are employed as well, to mimic multidimensional distributions to generate new instances, such as: Variational Autoencoders (VAE) Generative Adversarial Networks (GAN) ML is also used for time series analysis and prediction, which is less relevant to HEP but is useful when quality of data and compute monitoring is essential, specially when time is an important aspect, such as, in event reconstruction. Targeted areas for Machine Learning Applications and Research for HEP Detector Simulation: New particles are discovered by comparing the recorded collision data with the predictions from Standard Model/beyond-standard Model physics. Detector Simulator, such as GEANT, help in simulation of particle trajectory and compare with the recorded data. The detector response along with known particle-matter iteration results, one can proceed to discover new particles. The HL-LHC would require simulation of up to trillions of events, that may help in testing the hypothesis. Simulation one proton-proton collision for LHC takes several minutes, which in addition to higher computational resource requirements, would scale many fold for HL-LHC simulations. Read-time analysis and Triggering: Object Reconstruction, Identification and Calibration: End-to-end Deep Learning: Sustainable Matrix Element Method: Matrix Element Machine Learning Method: Learning the Standard Model: Theory Applications: Uncertainity Assignment: Monitoring the Detectors, Hardware Anomaly and Preemptive Maintenance: Computing Resource Optimization and Control of Networks and Productive Workflows: HEP Machine Learning Software Hardware Resources and Computing Constraints HEP ML Roadmap (2017-2022)</summary></entry></feed>