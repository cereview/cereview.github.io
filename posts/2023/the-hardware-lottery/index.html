<!DOCTYPE html>
<html lang="en">
<title>The Hardware Lottery | amitabh yadav</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="author" content="Amitabh Yadav">
<link rel="canonical" href="http://localhost:4000/posts/2023/the-hardware-lottery/">

<link rel="stylesheet" href="/assets/css/index.css">

<style> 
#topbtn {
  display: none;
  position: fixed;
  bottom: 55px;
  right: 1em;
  z-index: 99;
  font-size: 18px;
  border: solid black 0.5pt;
  outline: none;
  background-color: rgba(186, 186, 186, 0.25);
  color: black;
  cursor: pointer;
  padding: 8px;
  border-radius: 6px;
}

// collapsible scripts 
.collapsible {

}

.collapsible::before {
  content: "▸ " ;//"&#x25BA; ";
}

.collapsibleactive, .collapsible:hover {
  //background-color: #555;
  border-bottom: dotted 0.5pt red; 
}

.collapsibleactive::before {
  content: "▾ ";//"&#x25BC; ";
}

.collapsiblecontent {
  padding: 0 5px;
  display: none;
  overflow: hidden;
  background-color: white;
}
</style>
<link rel="alternate" href="/feed.xml" type="application/atom+xml" title="amitabh yadav">

<link rel="stylesheet" href="/assets/katex/katex.min.css">
<script defer src="/assets/katex/katex.min.js"></script>
<script defer src="/assets/katex/contrib/auto-render.min.js" onload="renderMathInElement(document.body)"></script>

<script src="//amitabhydv.disqus.com/embed.js" async></script>








<article>
<br><br><br><br><br><br>
  <header>
  <strong><span class="cursor-thick">|</span> <a href="/posts/2023/the-hardware-lottery/" style="text-decoration: none; font-size: 20pt; border-bottom: none;">The Hardware Lottery</a></strong></br><span><time datetime="2023-01-24T00:00:00+01:00" style="font-family: monospace;">January 24, 2023</time>  <span style="float:right; font-family: monospace; background-color: rgba(255,255,255, 255); color: #16407f" class="no-print">[ <a href="/">see all posts</a> ]</span></span>
</header>

  <p>Sara Hooker, <a href="https://arxiv.org/abs/2009.06489">“The hardware lottery”</a>, Commun. ACM 64, 12, December 2021, pp. 58-65, https://doi.org/10.1145/3467017</p>

<h4 id="synopsis">Synopsis</h4>
<p>The term, Hardware Lottery, is used to define the unintentional prejudice observed historically towards certain research ideas in computer science that succeed, not because they were far superior to others but due to how well-suited they were for the available hardware and software of the time, often creating ‘noise’ to the interpretability of new research directions. The domain-specific era is even more prone to this, where the hardware landscape is getting even more fragmented. Figure 1 demonstrates this. Writing software for each system is expensive and even then chances are only those companies would succeed whose technologies can successfully be adopted to commercial applications. Specialised hardware can be replaced with the advent of newer technologies, therefore scalability and adoptability is critical to a hardware development infrastructure.</p>

<p><img src="/assets/images/ai_accelerators_2022.png" alt="Figure 1: Peak performance vs peak power of publically announced accelerators in 2022" />
Figure 1: Peak performance vs peak power of publically announced accelerators in 2022 <a href="https://www.arxiv.org/abs/2210.04055">[arxiv:2210.04055]</a></p>

<h4 id="strengths">Strengths</h4>
<p>The numerous examples from history from Edison’s Phonograph to modern GPUs is an eye opener and provides a broader perspective to think about the larger picture of computing hardware landscape. The presented solutions are agreed upon propositions in the research community. The emphasis on revisiting FPGAs and CGRAs for harmonising hardware development, developing better profiling tools and using AI for design space exploration is something I agree with. Secondly, just like hardware is being evolved to be highly parameterized, parameterized DSLs for reproducing results on different hardware is necessary (functional programming, and a higher abstraction level of application specification can help: e.g. Code Generating (Explainable) AI for hardware backends).</p>

<h4 id="weaknesses">Weaknesses</h4>
<p>The article highlights the issues that cost, limited availability, and the lack of standardisation in hardware, will make it difficult for researchers to replicate or reproduce results. One potential missing element in the article is the discussion that the difficulties in obtaining hardware resources also limits the number of researchers working on certain problems, and slowing down of progress in certain areas. It leads to a lack of diversity in the types of research being conducted, as researchers may be limited to working on problems that can be solved with the hardware they have available (not to mention, Gartner Hype Cycle). Additionally, an obstacle is technology not being available for researchers from underprivileged countries, institutions or backgrounds (IC production is Expensive!). Lastly, it could also be discussed that standardising a set of norms for developers (in academia and industry) would help mitigate the fragmented landscape, e.g. with the open-source hardware platform like RISC-V, and compiler infrastructure like LLVM/MLIR.</p>

<h4 id="thoughts">Thoughts</h4>
<p>Domain-Specific era didn’t dawn until Moore’s Law and Dennard Scaling stopped to apply. It can be worth investigating the next research idea (after-domain specific) and lay a path for its integration in future, which might not apply today for immediate adoption but can be simply available when needed [<span style="color:blue;">1</span>, <span style="color:blue;">2</span>]. This can be an expensive endeavour and may not succeed - “being too early is the same as being wrong” - so caution must be exercised. For academic research, some areas include, photonic accelerator, synaptic transistor, memristor array, new memory technology, neuromorphic etc. for hardware; Neurosymbolic AI, Formal Methods etc. for software. Secondly, while it is true presently that having specialised hardware can greatly accelerate the training of deep learning models, there are also other ways to address the issue of creating uniformity in the domain-specific landscape, such as algorithmic research to use general purpose hardware (not everything needs DL), using cloud computing resources, and creating cross-DSL(domain-specific language) compilers, DSE tools and open-source solutions. Lastly, one positive impact in the DS-era would be that small teams and independent researchers would be making breakthroughs with more limited resources through the use of creative solutions and open-source tools.</p>

<h4 id="takeaways">Takeaways</h4>
<p>Hardware development maintained its course ever since the 90s, however software developed fast (number of ML publications since 2005). This led to hardware being abstracted away completely by software. With increasing software complexity, hardware will be a key factor that would dictate the success for a research idea [<span style="color:blue;">3</span>]. Whereas for the hardware competing in the ‘bigger is better’ race, history tells us, “a deficiency in any one number of factors dooms an endeavour to failure.” A critical understanding of the hardware/software landscape, their compatibility and co-designability is therefore elemental.</p>
<ul>
  <li>Large new neural networks would need faster development time for corresponding accelerators, some architectures would need even more constrained design than scaling systolic arrays/vector architectures e.g. [<span style="color:blue;">4</span>, <span style="color:blue;">5</span>, <span style="color:blue;">6</span>] and [<span style="color:blue;">7</span>].</li>
  <li>Accelerating development time and cost is essential for hardware engineers. Ideas like [<span style="color:blue;">8</span>, <span style="color:blue;">9</span>] didn’t succeed because the cost of iteration was too high. For software, developers need to come up with creative solutions to adapt to the current norm of accelerators.</li>
  <li>Looking ahead is essential to progress, ideas respawn when the time is right e.g. the case of deep neural networks in software and [<span style="color:blue;">10</span>, <span style="color:blue;">11</span>] for corresponding hardware. Additionally, DNNs may not be the only way forward, at some point a new technology may spawn (e.g. something that better resembles the human brain) and we may have to redo everything. Scaling and Adaptability here needs more emphasis.</li>
  <li>From an economist’s perspective, the law of supply and demand is at play here. In one line, “Necessity is the mother of invention”.</li>
</ul>

<h4 id="favourite-bits">Favourite bits</h4>
<p>“Our own cognitive intelligence is both hardware and software [a domain-specific computer].”</p>

<p>“[accelerators] Happy families are all alike, unhappy families are unhappy in their own way.”</p>

<p>“[bigger is better race] An apt metaphor is that we may be trying to build a ladder to the moon.”</p>

<p>“Scientific progress occurs when there is a confluence of factors which allows scientists to overcome the ‘stickiness’ of the existing paradigm.”</p>

<p>“Registering what differs from our expectations, remains a key catalyst in driving new scientific discoveries.”</p>

<p>“[computer chip is] inscribing words on grains of sand.”</p>

<h4 id="suggested-reading">Suggested Reading</h4>
<p>Science in the age of selfies [<span style="color:blue;">12</span>].</p>

<h4 id="case-study-of-isa">Case study of ISA</h4>
<p>Krste Asanovic once said, ‘Don’t make your own ISAs.’ This is partly true in the sense that an incompatible ISA without existing software would never make it to upstream. We have observed the success of two proprietary ISAs so far, x86_64 and ARM32/64, asserting their dominance in desktop/server and mobile markets, respectively due to them being early and developing their own software ecosystem. Other ISAs of the time VAX, MIPS, SPARC etc became only moderately successful (and are now out of circulation). Why is it partly true? Because new systems, like Graphcore and Cerebras are bringing enough performance (in addition to marketing) to the table to compete in the HPC space.</p>

<h3 id="references">References</h3>
<div style="font-size:11pt">
[<span style="color:blue;">1</span>] Sutton, R. The bitter lesson, 2019. URL http://www.incompleteideas.net/IncIdeas/BitterLesson.html.<br />

[<span style="color:blue;">2</span>] Welling, M. Dowestill need modelsorjust more data and compute?, 2019. URL shorturl. at/qABIY.<br />

[<span style="color:blue;">3</span>] Barham, P. and Isard, M. Machine learning systems are stuck in a rut. In Proceedings of the Workshop on Hot Topics in Operating Systems, HotOS ’19, pp. 177–183, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367271. doi: 10.1145/3317550.3321441. URL https://doi.org/10.1145/3317550.3321441.<br />

[<span style="color:blue;">4</span>] Hooker, S., Courville, A., Clark, G., Dauphin, Y., and Frome, A. What Do Compressed Deep Neural Networks Forget? arXiv e-prints, art. arXiv:1911.05248, November 2019.<br />

[<span style="color:blue;">5</span>] Gale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks, 2019.<br />

[<span style="color:blue;">6</span>] Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the Lottery: Making All Tickets Winners. arXiv e-prints, November 2019.<br />

[<span style="color:blue;">7</span>] Zhen, D., Yao, Z., Gholami, A., Mahoney, M., and Keutzer, K. Hawq: Hessian aware quantization of neural networks with mixedprecision, 10 2019.<br />

[<span style="color:blue;">8</span>] Kingsbury, B., Morgan, N., and Wawrzynek, J. Hipnet-1: A highly pipelined architecture for neural network training, 03 1998.<br />

[<span style="color:blue;">9</span>] Sackinger, E., Boser, B. E., Bromley, J., LeCun, Y., and Jackel, L. D. Application of the anna neural network chip to high-speed character recognition. IEEE Transactions on Neural Networks, 3(3):498–505, 1992.<br />

[<span style="color:blue;">10</span>] Hinton, G. E. and Anderson, J. A. Parallel Models of Associative Memory. L. Erlbaum Associates Inc., USA, 1989. ISBN 080580269X.<br />

[<span style="color:blue;">11</span>] Rumelhart, D. E., McClelland, J. L., and PDP Research Group, C. (eds.). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations. MIT Press, Cambridge, MA, USA, 1986. ISBN 026268053X.<br />

[<span style="color:blue;">12</span>] Geman, Donald, and Stuart Geman. "Science in the age of selfies." Proceedings of the National Academy of Sciences 113.34 (2016): 9384-9387. <br />
</div>

<p><code class="language-plaintext highlighter-rouge">[This page is optimized for printing. Please consider saving paper by refraining from printing unless absolutely necessary.]</code></p>

  <br> <div class="no-print">
  <hr style="text-align=left; background-color:red;">
  
	<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    	this.page.url = http://localhost:4000/posts/2023/the-hardware-lottery/;  // Replace PAGE_URL with your page's canonical URL variable
    	this.page.identifier = http://localhost:4000/posts/2023/the-hardware-lottery/; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://amitabhydv.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments.</a></noscript>

  
  </div>
  
</article>


	<div style="position: fixed; top: -0.6pt; right:1em; background: white; border: solid black 0.5pt; color: black;" class="no-print">&nbsp; <a href="/" style="color: inherit;">blog <span style="font-weight: 1000; font-size: 15px; color: red;">| </span>amitabh yadav</a> &nbsp; </div>



  <button onclick="topFunction()" id="topbtn" title="Go to top" class="no-print">Go to Top</button>

<footer style="font-family:monospace;" class="no-print">
  <div>&copy; <script type="text/javascript"> document.write((new Date().getFullYear())); </script> Amitabh Yadav</div> <!-- <div>It's not right but the thing is, it's not even wrong!</div>--> 
  <nav><a href="/" >[ all posts ]</a><a href="/categories/" >[ categories ]</a><a href="/about" >[ about ]</a></nav>

</footer>


<!-- Go to Top Scripts -->
<script>
//Get the button
var mybutton = document.getElementById("topbtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
</script>

<!-- Collapsible Scripts-->
<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("collapsibleactive");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

</html>
